{
    "id": "5193653c-92c9-4f34-8370-275fdc82cb02",
    "node_id": "d35f7363-4d0a-40a3-b52d-d92ff1050c33",
    "node_name": "test-project",
    "end_time": "2018-11-26T16:18:42Z",
    "status": "failed",
    "controls": null,
    "environment": "gcp-api",
    "version": "3.0.52",
    "platform": {
        "name": "gcp",
        "release": "google-api-client-v0.23.9"
    },
    "statistics": {
        "duration": 0.3005533
    },
    "profiles": [
        {
            "name": "cis-gcp-benchmark-level2",
            "title": "CIS Google Cloud Platform Foundation Benchmark Level 2",
            "maintainer": "",
            "copyright": "Chef Software, Inc.",
            "copyright_email": "support@chef.io",
            "license": "",
            "summary": "CIS Google Cloud Platform Foundation Benchmark Level 2",
            "version": "1.0.0-1",
            "owner": "",
            "supports": [],
            "depends": [
                {
                    "name": "inspec-gcp",
                    "url": "https://github.com/inspec/inspec-gcp/archive/master.tar.gz",
                    "path": "",
                    "git": "",
                    "branch": "",
                    "tag": "",
                    "commit": "",
                    "version": "",
                    "supermarket": "",
                    "github": "",
                    "compliance": "",
                    "status": "loaded",
                    "skip_message": ""
                }
            ],
            "sha256": "e91e3f85a7fea90e7390f48d18368f69c17dc151e74cea9ed137d9a18bdad760",
            "groups": [],
            "controls": [
                {
                    "id": "cis-gcp-benchmark-database-6.1",
                    "code": "control 'cis-gcp-benchmark-database-6.1' do\n  impact 1.0\n  title 'Ensure that Cloud SQL database instance requires all incoming connections to use SSL'\n  desc '\n  It is recommended to enforce all incoming connections to SQL database instance to use SSL.\n\n  Rationale:\n  SQL database connections if successfully trapped (MITM); can reveal sensitive data like credentials, database\n  queries, query outputs etc. For security, it is recommended to always use SSL encryption when connecting to your\n  instance. This recommendation is applicable for Postgresql, MySql generation 1 and MySql generation 2 Instances.\n  '\n  tag cis: 'gcp:6.1'\n  tag level: 1\n\n  google_sql_database_instances(project: gcp_project_id).instance_names.each do |instance_name|\n    describe google_sql_database_instance(project: gcp_project_id, database: instance_name) do\n      it { should have_ip_configuration_require_ssl }\n    end\n  end\nend\n",
                    "desc": "It is recommended to enforce all incoming connections to SQL database instance to use SSL.\n\nRationale:\nSQL database connections if successfully trapped (MITM); can reveal sensitive data like credentials, database\nqueries, query outputs etc. For security, it is recommended to always use SSL encryption when connecting to your\ninstance. This recommendation is applicable for Postgresql, MySql generation 1 and MySql generation 2 Instances.",
                    "impact": 1,
                    "title": "Ensure that Cloud SQL database instance requires all incoming connections to use SSL",
                    "source_location": {
                        "ref": "controls/database_6_1.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-database-6.2",
                    "code": "control 'cis-gcp-benchmark-database-6.2' do\n  impact 1.0\n  title 'Ensure that Cloud SQL database Instances are not open to the world'\n  desc '\n  Database Server should accept connections only from trusted Network(s)/IP(s) and restrict access from the world.\n\n  Rationale:\n  To minimize attack surface on a Database server Instance, only trusted/known and required IP(s) should be\n  white-listed to connect to it.\n  Authorized network should not have IPs/networks configured to 0.0.0.0 or /0 which will allow access to the\n  instance from anywhere in the world.\n  '\n  tag cis: 'gcp:6.2'\n  tag level: 1\n\n  google_sql_database_instances(project: gcp_project_id).instance_names.each do |instance_name|\n    describe google_sql_database_instance(project: gcp_project_id, database: instance_name) do\n      its('authorized_networks') { should_not include '0.0.0.0/0' }\n    end\n  end\nend\n",
                    "desc": "Database Server should accept connections only from trusted Network(s)/IP(s) and restrict access from the world.\n\nRationale:\nTo minimize attack surface on a Database server Instance, only trusted/known and required IP(s) should be\nwhite-listed to connect to it.\nAuthorized network should not have IPs/networks configured to 0.0.0.0 or /0 which will allow access to the\ninstance from anywhere in the world.",
                    "impact": 1,
                    "title": "Ensure that Cloud SQL database Instances are not open to the world",
                    "source_location": {
                        "ref": "controls/database_6_2.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-iam-1.1",
                    "code": "control 'cis-gcp-benchmark-iam-1.1' do\n  impact 1.0\n  title 'Use corporate login credentials instead of Gmail accounts.'\n  desc '\n  Gmail accounts are personally created and controllable accounts. Organizations seldom have any control over them.\n  Thus, it is recommended that you use fully managed corporate Google accounts for increased visibility, auditing,\n  and control over access to Cloud Platform resources.\n  '\n  tag cis: 'gcp:1.1'\n  tag level: 1\n\n  google_project_iam_bindings(project: gcp_project_id).iam_binding_roles.each do |iam_binding_role|\n    describe google_project_iam_binding(project: gcp_project_id, role: iam_binding_role) do\n      its('members.to_s') { should_not match 'gmail.com' }\n    end\n  end\nend\n",
                    "desc": "Gmail accounts are personally created and controllable accounts. Organizations seldom have any control over them.\nThus, it is recommended that you use fully managed corporate Google accounts for increased visibility, auditing,\nand control over access to Cloud Platform resources.",
                    "impact": 1,
                    "title": "Use corporate login credentials instead of Gmail accounts.",
                    "source_location": {
                        "ref": "controls/iam_1_1.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-iam-1.3",
                    "code": "control 'cis-gcp-benchmark-iam-1.3' do\n  impact 1.0\n  title 'User managed service account should not have user managed keys.'\n  desc '\n  Anyone who has access to the keys will be able to access resources through the service account. GCP-managed keys are\n  used by Cloud Platform services such as App Engine and Compute Engine. These keys cannot be downloaded. Google will\n  keep the keys and automatically rotate them on an approximately weekly basis. User-managed keys are created,\n  downloadable, and managed by users. They expire 10 years from creation.\n\n  For user-managed keys, User have to take ownership of key management activities which includes:\n  * Key storage\n  * Key distribution\n  * Key revocation\n  * Key rotation\n  * Protecting the keys from unauthorized users\n  * Key recovery Even after owners precaution, Keys can be easily leaked by common development malpractices like\n    checking keys into the source code or leaving them in Downloads directory, antecedently leaving them on support blogs/channels.\n\n  It is recommended to prevent use of User-managed service account keys.\n  '\n  tag cis: 'gcp:1.3'\n  tag level: 1\n\n  google_service_accounts(project: gcp_project_id).service_account_names.each do |service_account_name|\n    describe google_service_account(name: service_account_name) do\n      it { should_not have_user_managed_keys }\n    end\n  end\nend\n",
                    "desc": "Anyone who has access to the keys will be able to access resources through the service account. GCP-managed keys are\nused by Cloud Platform services such as App Engine and Compute Engine. These keys cannot be downloaded. Google will\nkeep the keys and automatically rotate them on an approximately weekly basis. User-managed keys are created,\ndownloadable, and managed by users. They expire 10 years from creation.\n\nFor user-managed keys, User have to take ownership of key management activities which includes:\n* Key storage\n* Key distribution\n* Key revocation\n* Key rotation\n* Protecting the keys from unauthorized users\n* Key recovery Even after owners precaution, Keys can be easily leaked by common development malpractices like\n  checking keys into the source code or leaving them in Downloads directory, antecedently leaving them on support blogs/channels.\n\nIt is recommended to prevent use of User-managed service account keys.",
                    "impact": 1,
                    "title": "User managed service account should not have user managed keys.",
                    "source_location": {
                        "ref": "controls/iam_1_3.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-iam-1.4",
                    "code": "control 'cis-gcp-benchmark-iam-1.4' do\n  impact 1.0\n  title 'Ensure that ServiceAccount has no Admin privileges.'\n  desc '\n  A service account is a special Google account that belongs to your application or a VM, instead of to an\n  individual end user. Your application uses the service account to call the Google API of a service, so that\n  the users aren\\'t directly involved, It\\'s recommended not to use admin access for ServiceAccount.\n\n  Rationale:\n  Service accounts represent service-level security of the Resources (application or a VM) which can be determined\n  by the roles assigned to it. Enrolling ServiceAccount with Admin rights gives full access to assigned application or a\n  VM, ServiceAccount Access holder can perform critical actions like delete, update change settings etc. without the\n  intervention of user, So It\\'s recommended not to have Admin rights.\n  This recommendation is applicable only for User-Managed user created service account (Service account with\n  nomenclature: SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com).\n  '\n  tag cis: 'gcp:1.4'\n  tag level: 1\n\n  google_project_iam_bindings(project: gcp_project_id).where(iam_binding_role: /admin/).iam_binding_roles.each do |role|\n    describe google_project_iam_binding(project: gcp_project_id, role: role) do\n      its('members.to_s') { should_not match \"@#{gcp_project_id}.iam.gserviceaccount.com\" }\n    end\n  end\n\n  google_project_iam_bindings(project: gcp_project_id).where(iam_binding_role: /Admin/).iam_binding_roles.each do |role|\n    describe google_project_iam_binding(project: gcp_project_id, role: role) do\n      its('members.to_s') { should_not match \"@#{gcp_project_id}.iam.gserviceaccount.com\" }\n    end\n  end\n\n  google_project_iam_bindings(project: gcp_project_id).where(iam_binding_role: 'roles/editor').iam_binding_roles.each do |role|\n    describe google_project_iam_binding(project: gcp_project_id, role: role) do\n      its('members.to_s') { should_not match \"@#{gcp_project_id}.iam.gserviceaccount.com\" }\n    end\n  end\n\n  google_project_iam_bindings(project: gcp_project_id).where(iam_binding_role: 'roles/owner').iam_binding_roles.each do |role|\n    describe google_project_iam_binding(project: gcp_project_id, role: role) do\n      its('members.to_s') { should_not match \"@#{gcp_project_id}.iam.gserviceaccount.com\" }\n    end\n  end\nend\n",
                    "desc": "A service account is a special Google account that belongs to your application or a VM, instead of to an\nindividual end user. Your application uses the service account to call the Google API of a service, so that\nthe users aren't directly involved, It's recommended not to use admin access for ServiceAccount.\n\nRationale:\nService accounts represent service-level security of the Resources (application or a VM) which can be determined\nby the roles assigned to it. Enrolling ServiceAccount with Admin rights gives full access to assigned application or a\nVM, ServiceAccount Access holder can perform critical actions like delete, update change settings etc. without the\nintervention of user, So It's recommended not to have Admin rights.\nThis recommendation is applicable only for User-Managed user created service account (Service account with\nnomenclature: SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com).",
                    "impact": 1,
                    "title": "Ensure that ServiceAccount has no Admin privileges.",
                    "source_location": {
                        "ref": "controls/iam_1_4.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-iam-1.5",
                    "code": "control 'cis-gcp-benchmark-iam-1.5' do\n  impact 1.0\n  title 'Ensure that IAM users are not assigned Service Account User role at project level.'\n  desc '\n  It is recommended to assign Service Account User (iam.serviceAccountUser) role to a user for a\n  specific service account rather than assigning the role to a user at project level.\n\n  Rationale:\n  Granting the iam.serviceAccountUser role to a user for a project gives the user access to all service accounts in\n  the project, including service accounts that may be created in the future. This can result into elevation of\n  privileges by using service accounts and corresponding Compute Engine instances.\n  In order to implement least privileges best practices, IAM users should not be assigned Service Account User role\n  at project level. Instead iam.serviceAccountUser role should be assigned to a user for a specific service account\n  giving a user access to the service account.\n  '\n  tag cis: 'gcp:1.5'\n  tag level: 1\n\n  describe google_project_iam_bindings(project: gcp_project_id).where(iam_binding_role: 'roles/iam.serviceAccountUser') do\n    it { should_not exist }\n  end\nend\n",
                    "desc": "It is recommended to assign Service Account User (iam.serviceAccountUser) role to a user for a\nspecific service account rather than assigning the role to a user at project level.\n\nRationale:\nGranting the iam.serviceAccountUser role to a user for a project gives the user access to all service accounts in\nthe project, including service accounts that may be created in the future. This can result into elevation of\nprivileges by using service accounts and corresponding Compute Engine instances.\nIn order to implement least privileges best practices, IAM users should not be assigned Service Account User role\nat project level. Instead iam.serviceAccountUser role should be assigned to a user for a specific service account\ngiving a user access to the service account.",
                    "impact": 1,
                    "title": "Ensure that IAM users are not assigned Service Account User role at project level.",
                    "source_location": {
                        "ref": "controls/iam_1_5.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_project_iam_bindings with iam_binding_role == \"roles/iam.serviceAccountUser\" ",
                            "run_time": 0.000165617,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-iam-1.6",
                    "code": "control 'cis-gcp-benchmark-iam-1.6' do\n  impact 1.0\n  title 'Ensure user-managed/external keys for service accounts are rotated every 90 days or less.'\n  desc '\n  Service Account keys consist of a key ID (Private_key_Id) and Private key, which are used to sign programmatic\n  requests that you make to Google cloud services accessible to that particular Service account. It is recommended\n  that all Service Account keys are regularly rotated.\n  '\n  tag cis: 'gcp:1.6'\n  tag level: 1\n\n  google_service_accounts(project: gcp_project_id).service_account_names.each do |name|\n    describe google_service_account_keys(service_account: name).where { valid_after_time < Time.now - 60*60*24*90 } do\n      it { should_not exist }\n    end\n  end\nend\n",
                    "desc": "Service Account keys consist of a key ID (Private_key_Id) and Private key, which are used to sign programmatic\nrequests that you make to Google cloud services accessible to that particular Service account. It is recommended\nthat all Service Account keys are regularly rotated.",
                    "impact": 1,
                    "title": "Ensure user-managed/external keys for service accounts are rotated every 90 days or less.",
                    "source_location": {
                        "ref": "controls/iam_1_6.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-iam-1.7",
                    "code": "control 'cis-gcp-benchmark-iam-1.7' do\n  only_if { cis_level == 2 }\n  impact 1.0\n  title 'Ensure that Separation of duties is enforced while assigning service account related roles to users.'\n  desc '\n  A service account is a special Google account that belongs to your application or a VM, instead of to an\n  individual end user. Your application uses the service account to call the Google API of a service, so that\n  the users aren\\'t directly involved, It\\'s recommended not to use admin access for ServiceAccount.\n\n  Rationale:\n  Service accounts represent service-level security of the Resources (application or a VM) which can be determined\n  by the roles assigned to it. Enrolling ServiceAccount with Admin rights gives full access to assigned application or a\n  VM, ServiceAccount Access holder can perform critical actions like delete, update change settings etc. without the\n  intervention of user, So It\\'s recommended not to have Admin rights.\n  This recommendation is applicable only for User-Managed user created service account (Service account with\n  nomenclature: SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com).\n  It is recommended that principle of Separation of duties is enforced while assigning service account\n  related roles to users.\n\n  Rationale:\n  Built-in/Predefined IAM role Service Account admin allows user/identity to create, delete, manage service\n  account(s). Built-in/Predefined IAM role Service Account User allows user/identity (with adequate privileges on\n  Compute and App Engine) to assign service account(s) to Apps/Compute Instances.\n\n  Separation of duties is the concept of ensuring that one individual does not have all necessary permissions to be\n  able to complete a malicious action. In Cloud IAM - service accounts, this could be an action such as using a service\n  account to access resources that user should not normally have access to. Separation of duties is a business control\n  typically used in larger organizations, meant to help avoid security or privacy incidents and errors. It is\n  considered best practice.\n\n  Any user(s) should not have Service Account Admin and Service Account User, both roles assigned at a time.\n  '\n  tag cis: 'gcp:1.7'\n  tag level: 2\n\n  # First get the users having the Service Account Admin role (normally fewer than those having the equivalent User role)\n  service_account_administrators = google_project_iam_binding(project: gcp_project_id, role: 'roles/iam.serviceAccountAdmin').members\n  # Now check whether Users contain Admins\n  describe google_project_iam_binding(project: gcp_project_id, role: 'roles/iam.serviceAccountUser') do\n    service_account_administrators.each do |admin|\n      its('members.to_s') { should_not match admin }\n    end\n  end\nend\n",
                    "desc": "A service account is a special Google account that belongs to your application or a VM, instead of to an\nindividual end user. Your application uses the service account to call the Google API of a service, so that\nthe users aren't directly involved, It's recommended not to use admin access for ServiceAccount.\n\nRationale:\nService accounts represent service-level security of the Resources (application or a VM) which can be determined\nby the roles assigned to it. Enrolling ServiceAccount with Admin rights gives full access to assigned application or a\nVM, ServiceAccount Access holder can perform critical actions like delete, update change settings etc. without the\nintervention of user, So It's recommended not to have Admin rights.\nThis recommendation is applicable only for User-Managed user created service account (Service account with\nnomenclature: SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com).\nIt is recommended that principle of Separation of duties is enforced while assigning service account\nrelated roles to users.\n\nRationale:\nBuilt-in/Predefined IAM role Service Account admin allows user/identity to create, delete, manage service\naccount(s). Built-in/Predefined IAM role Service Account User allows user/identity (with adequate privileges on\nCompute and App Engine) to assign service account(s) to Apps/Compute Instances.\n\nSeparation of duties is the concept of ensuring that one individual does not have all necessary permissions to be\nable to complete a malicious action. In Cloud IAM - service accounts, this could be an action such as using a service\naccount to access resources that user should not normally have access to. Separation of duties is a business control\ntypically used in larger organizations, meant to help avoid security or privacy incidents and errors. It is\nconsidered best practice.\n\nAny user(s) should not have Service Account Admin and Service Account User, both roles assigned at a time.",
                    "impact": 1,
                    "title": "Ensure that Separation of duties is enforced while assigning service account related roles to users.",
                    "source_location": {
                        "ref": "controls/iam_1_7.rb",
                        "line": 8
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Project IAM Binding roles/iam.serviceAccountUser ",
                            "run_time": 0.000048951,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-iam-1.8",
                    "code": "control 'cis-gcp-benchmark-iam-1.8' do\n  impact 1.0\n  title 'Ensure Encryption keys are rotated within a period of 365 days.'\n  desc '\n  Google Cloud Key Management Service stores cryptographic keys in a hierarchical structure designed for useful\n  and elegant access control management. Access to resources.\n  The format for the rotation schedule depends on the client library that is used. For the gcloud command-line tool,\n  the next rotation time must be in ISO or RFC3339 format, and the rotation period must be in the form INTEGER[UNIT],\n  where units can be one of seconds (s), minutes (m), hours (h) or days (d).\n\n  Rationale:\n  Set a key rotation period and starting time, A key can be created with a specified rotation period, which is the\n  time between when new key versions are generated automatically. A key can also be created with a specified\n  next rotation time. A key is a named object representing a cryptographic key used for a specific purpose. The key\n  material, the actual bits used for encryption, can change over time as new key versions are created.\n  A key is used to protect some corpus of data. You could encrypt a collection of files with the same key, and people\n  with decrypt permissions on that key would be able to decrypt those files. Hence it\\'s necessary to make sure\n  rotation period is set to specific time.\n  '\n  tag cis: 'gcp:1.8'\n  tag level: 1\n\n  google_compute_regions(project: gcp_project_id).region_names.each do |region_name|\n    google_kms_key_rings(project: gcp_project_id, location: region_name).key_ring_names.each do |key_ring|\n      google_kms_crypto_keys(project: gcp_project_id, location: region_name, key_ring_name: key_ring).crypto_key_names.each do |key_name|\n        describe google_kms_crypto_key(project: gcp_project_id, location: region_name, key_ring_name: key_ring, name: key_name) do\n          its('next_rotation_time_date') { should be <= Time.now + 365*60*60*24 }\n          its('rotation_period_seconds') { should be <= 365*60*60*24 }\n        end\n      end\n    end\n  end\nend\n",
                    "desc": "Google Cloud Key Management Service stores cryptographic keys in a hierarchical structure designed for useful\nand elegant access control management. Access to resources.\nThe format for the rotation schedule depends on the client library that is used. For the gcloud command-line tool,\nthe next rotation time must be in ISO or RFC3339 format, and the rotation period must be in the form INTEGER[UNIT],\nwhere units can be one of seconds (s), minutes (m), hours (h) or days (d).\n\nRationale:\nSet a key rotation period and starting time, A key can be created with a specified rotation period, which is the\ntime between when new key versions are generated automatically. A key can also be created with a specified\nnext rotation time. A key is a named object representing a cryptographic key used for a specific purpose. The key\nmaterial, the actual bits used for encryption, can change over time as new key versions are created.\nA key is used to protect some corpus of data. You could encrypt a collection of files with the same key, and people\nwith decrypt permissions on that key would be able to decrypt those files. Hence it's necessary to make sure\nrotation period is set to specific time.",
                    "impact": 1,
                    "title": "Ensure Encryption keys are rotated within a period of 365 days.",
                    "source_location": {
                        "ref": "controls/iam_1_8.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-iam-1.9",
                    "code": "control 'cis-gcp-benchmark-iam-1.9' do\n  only_if { cis_level == 2 }\n  impact 1.0\n  title 'Ensure that Separation of duties is enforced while assigning KMS related roles to users.'\n  desc '\n  It is recommended that principle of Separation of duties is enforced while assigning KMS related roles to users.\n\n  Rationale:\n  Built-in/Predefined IAM role Cloud KMS Admin allows user/identity to create, delete, and manage service\n  account(s). Built-in/Predefined IAM role Cloud KMS CryptoKey Encrypter/Decrypter allows user/identity (with\n  adequate privileges on concerned resources) to encrypt and decrypt data at rest using encryption key(s).\n  Built-in/Predefined IAM role Cloud KMS CryptoKey Encrypter allows user/identity (with adequate privileges on\n  concerned resources) to encrypt data at rest using encryption key(s). Built- in/Predefined IAM role Cloud KMS\n  CryptoKey Decrypter allows user/identity (with adequate privileges on concerned resources) to decrypt data at\n  rest using encryption key(s).\n\n  Separation of duties is the concept of ensuring that one individual does not have all necessary permissions to be\n  able to complete a malicious action. In Cloud KMS, this could be an action such as using a key to access and decrypt\n  data that that user should not normally have access to. Separation of duties is a business control typically used in\n  larger organizations, meant to help avoid security or privacy incidents and errors. It is considered best practice.\n\n  Any user(s) should not have Cloud KMS Admin and any of the Cloud KMS CryptoKey Encrypter/Decrypter, Cloud KMS\n  CryptoKey Encrypter, Cloud KMS CryptoKey Decrypter roles assigned at a time.\n  '\n  tag cis: 'gcp:1.9'\n  tag level: 2\n\n  # First get the users having the KMS Admin role\n  kms_administrators = google_project_iam_binding(project: gcp_project_id, role: 'roles/cloudkms.admin').members\n  # Now check whether others contain Admins\n  describe google_project_iam_binding(project: gcp_project_id, role: 'roles/cloudkms.cryptoKeyEncrypterDecrypter') do\n    kms_administrators.each do |admin|\n      its('members.to_s') { should_not match admin }\n    end\n  end\n\n  describe google_project_iam_binding(project: gcp_project_id, role: 'roles/cloudkms.cryptoKeyEncrypter') do\n    kms_administrators.each do |admin|\n      its('members.to_s') { should_not match admin }\n    end\n  end\n\n  describe google_project_iam_binding(project: gcp_project_id, role: 'roles/cloudkms.cryptoKeyDecrypter') do\n    kms_administrators.each do |admin|\n      its('members.to_s') { should_not match admin }\n    end\n  end\nend\n",
                    "desc": "It is recommended that principle of Separation of duties is enforced while assigning KMS related roles to users.\n\nRationale:\nBuilt-in/Predefined IAM role Cloud KMS Admin allows user/identity to create, delete, and manage service\naccount(s). Built-in/Predefined IAM role Cloud KMS CryptoKey Encrypter/Decrypter allows user/identity (with\nadequate privileges on concerned resources) to encrypt and decrypt data at rest using encryption key(s).\nBuilt-in/Predefined IAM role Cloud KMS CryptoKey Encrypter allows user/identity (with adequate privileges on\nconcerned resources) to encrypt data at rest using encryption key(s). Built- in/Predefined IAM role Cloud KMS\nCryptoKey Decrypter allows user/identity (with adequate privileges on concerned resources) to decrypt data at\nrest using encryption key(s).\n\nSeparation of duties is the concept of ensuring that one individual does not have all necessary permissions to be\nable to complete a malicious action. In Cloud KMS, this could be an action such as using a key to access and decrypt\ndata that that user should not normally have access to. Separation of duties is a business control typically used in\nlarger organizations, meant to help avoid security or privacy incidents and errors. It is considered best practice.\n\nAny user(s) should not have Cloud KMS Admin and any of the Cloud KMS CryptoKey Encrypter/Decrypter, Cloud KMS\nCryptoKey Encrypter, Cloud KMS CryptoKey Decrypter roles assigned at a time.",
                    "impact": 1,
                    "title": "Ensure that Separation of duties is enforced while assigning KMS related roles to users.",
                    "source_location": {
                        "ref": "controls/iam_1_9.rb",
                        "line": 8
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Project IAM Binding roles/cloudkms.cryptoKeyEncrypterDecrypter ",
                            "run_time": 0.000044202,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Project IAM Binding roles/cloudkms.cryptoKeyEncrypter ",
                            "run_time": 0.000042705,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Project IAM Binding roles/cloudkms.cryptoKeyDecrypter ",
                            "run_time": 0.000047694,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.1",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.1' do\n  impact 1.0\n  title 'Ensure Stackdriver Logging is set to Enabled on Kubernetes Engine Clusters'\n  desc '\n  Stackdriver Logging is part of the Stackdriver suite of products in Google Cloud Platform. It includes storage for\n  logs, a user interface called the Logs Viewer, and an API to manage logs programmatically. Stackdriver Logging\n  lets you have Kubernetes Engine automatically collect, process, and store your container and system logs in a\n  dedicated, persistent datastore. Container logs are collected from your containers. System logs are collected from\n  the cluster\\'s components, such as docker and kubelet. Events are logs about activity in the cluster, such as the\n  scheduling of Pods.\n\n  Rationale:\n  By Enabling you will have container and system logs, Kubernetes Engine deploys a per- node logging agent that reads\n  container logs, adds helpful metadata, and then stores them. The logging agent checks for container logs in the\n  following sources:\n  - Standard output and standard error logs from containerized processes\n  - kubelet and container runtime logs\n  - Logs for system components, such as VM startup scripts\n\n  For events, Kubernetes Engine uses a Deployment in the kube-system namespace which automatically collects events\n  and sends them to Stackdriver Logging.\n\n  Stackdriver Logging is compatible with JSON and glog formats. Logs are stored for up to 30 days.\n  '\n  tag cis: 'gcp:7.1'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_logging_enabled }\n      end\n    end\n  end\nend\n",
                    "desc": "Stackdriver Logging is part of the Stackdriver suite of products in Google Cloud Platform. It includes storage for\nlogs, a user interface called the Logs Viewer, and an API to manage logs programmatically. Stackdriver Logging\nlets you have Kubernetes Engine automatically collect, process, and store your container and system logs in a\ndedicated, persistent datastore. Container logs are collected from your containers. System logs are collected from\nthe cluster's components, such as docker and kubelet. Events are logs about activity in the cluster, such as the\nscheduling of Pods.\n\nRationale:\nBy Enabling you will have container and system logs, Kubernetes Engine deploys a per- node logging agent that reads\ncontainer logs, adds helpful metadata, and then stores them. The logging agent checks for container logs in the\nfollowing sources:\n- Standard output and standard error logs from containerized processes\n- kubelet and container runtime logs\n- Logs for system components, such as VM startup scripts\n\nFor events, Kubernetes Engine uses a Deployment in the kube-system namespace which automatically collects events\nand sends them to Stackdriver Logging.\n\nStackdriver Logging is compatible with JSON and glog formats. Logs are stored for up to 30 days.",
                    "impact": 1,
                    "title": "Ensure Stackdriver Logging is set to Enabled on Kubernetes Engine Clusters",
                    "source_location": {
                        "ref": "controls/kubernetes_7_1.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.10",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.10' do\n  impact 1.0\n  title 'Ensure Basic Authentication is disabled on Kubernetes Engine Clusters'\n  desc '\n  Basic authentication allows a user to authenticate to the cluster with a username and password and it is stored\n  in plain text without any encryption. Disabling Basic authentication will prevent attacks like brute force. Its\n  recommended to use either client certificate or IAM for authentication.\n\n  Rationale:\n  When disabled, you will still be able to authenticate to the cluster with client certificate or IAM. A client\n  certificate is a base64-encoded public certificate used by clients to authenticate to the cluster endpoint. Disable\n  client certificate generation to create a cluster without a client certificate.\n  '\n  tag cis: 'gcp:7.10'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should_not have_basic_authorization }\n      end\n    end\n  end\nend\n",
                    "desc": "Basic authentication allows a user to authenticate to the cluster with a username and password and it is stored\nin plain text without any encryption. Disabling Basic authentication will prevent attacks like brute force. Its\nrecommended to use either client certificate or IAM for authentication.\n\nRationale:\nWhen disabled, you will still be able to authenticate to the cluster with client certificate or IAM. A client\ncertificate is a base64-encoded public certificate used by clients to authenticate to the cluster endpoint. Disable\nclient certificate generation to create a cluster without a client certificate.",
                    "impact": 1,
                    "title": "Ensure Basic Authentication is disabled on Kubernetes Engine Clusters",
                    "source_location": {
                        "ref": "controls/kubernetes_7_10.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.11",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.11' do\n  impact 1.0\n  title 'Ensure Network policy is enabled on Kubernetes Engine Cluster'\n  desc '\n  A network policy is a specification of how groups of pods are allowed to communicate with each other and other\n  network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic\n  is allowed to the selected pods. The Kubernetes Network Policy API allows the cluster administrator to specify\n  what pods are allowed to communicate with each other.\n\n  Rationale:\n  By default, pods are non-isolated; they accept traffic from any source. Pods become isolated by having a\n  NetworkPolicy that selects them. Once there is any NetworkPolicy in a namespace selecting a particular pod, that\n  pod will reject any connections that are not allowed by any NetworkPolicy. (Other pods in the namespace that are\n  not selected by any NetworkPolicy will continue to accept all traffic.)\n  '\n  tag cis: 'gcp:7.11'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_network_policy_enabled }\n      end\n    end\n  end\nend\n",
                    "desc": "A network policy is a specification of how groups of pods are allowed to communicate with each other and other\nnetwork endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic\nis allowed to the selected pods. The Kubernetes Network Policy API allows the cluster administrator to specify\nwhat pods are allowed to communicate with each other.\n\nRationale:\nBy default, pods are non-isolated; they accept traffic from any source. Pods become isolated by having a\nNetworkPolicy that selects them. Once there is any NetworkPolicy in a namespace selecting a particular pod, that\npod will reject any connections that are not allowed by any NetworkPolicy. (Other pods in the namespace that are\nnot selected by any NetworkPolicy will continue to accept all traffic.)",
                    "impact": 1,
                    "title": "Ensure Network policy is enabled on Kubernetes Engine Cluster",
                    "source_location": {
                        "ref": "controls/kubernetes_7_11.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.12",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.12' do\n  impact 1.0\n  title 'Ensure Kubernetes Cluster is created with Client Certificate enabled'\n  desc '\n  A client certificate is a base64-encoded public certificate used by clients to authenticate to the cluster endpoint.\n\n  Rationale:\n  If you disable client certificate generation to create a cluster without a client certificate. You will still be\n  able to authenticate to the cluster with basic auth or IAM. But basic auth allows a user to authenticate to the\n  cluster with a username and password which are stored in plain text without any encryption and might lead\n  brute force attacks.\n  '\n  tag cis: 'gcp:7.12'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_master_auth_client_key }\n      end\n    end\n  end\nend\n",
                    "desc": "A client certificate is a base64-encoded public certificate used by clients to authenticate to the cluster endpoint.\n\nRationale:\nIf you disable client certificate generation to create a cluster without a client certificate. You will still be\nable to authenticate to the cluster with basic auth or IAM. But basic auth allows a user to authenticate to the\ncluster with a username and password which are stored in plain text without any encryption and might lead\nbrute force attacks.",
                    "impact": 1,
                    "title": "Ensure Kubernetes Cluster is created with Client Certificate enabled",
                    "source_location": {
                        "ref": "controls/kubernetes_7_12.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.13",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.13' do\n  impact 1.0\n  title 'Ensure Kubernetes Cluster is created with Alias IP ranges enabled'\n  desc '\n  Google Cloud Platform Alias IP Ranges lets you assign ranges of internal IP addresses as aliases to a virtual\n  machine\\'s network interfaces. This is useful if you have multiple services running on a VM and you want to assign\n  each service a different IP address.\n\n  Rationale:\n  With Alias IPs ranges enabled, Kubernetes Engine clusters can allocate IP addresses from a CIDR block known to\n  Google Cloud Platform. This makes your cluster more scalable and allows your cluster to better interact with other\n  GCP products and entities. Using Alias IPs has several benefits:\n  - Pod IPs are reserved within the network ahead of time, which prevents conflict with other compute resources.\n  - The networking layer can perform anti-spoofing checks to ensure that egress traffic is not sent with arbitrary source IPs.\n  - Firewall controls for Pods can be applied separately from their nodes.\n  - Alias IPs allow Pods to directly access hosted services without using a NAT gateway.\n  '\n  tag cis: 'gcp:7.13'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_ip_alias_enabled }\n      end\n    end\n  end\nend\n",
                    "desc": "Google Cloud Platform Alias IP Ranges lets you assign ranges of internal IP addresses as aliases to a virtual\nmachine's network interfaces. This is useful if you have multiple services running on a VM and you want to assign\neach service a different IP address.\n\nRationale:\nWith Alias IPs ranges enabled, Kubernetes Engine clusters can allocate IP addresses from a CIDR block known to\nGoogle Cloud Platform. This makes your cluster more scalable and allows your cluster to better interact with other\nGCP products and entities. Using Alias IPs has several benefits:\n- Pod IPs are reserved within the network ahead of time, which prevents conflict with other compute resources.\n- The networking layer can perform anti-spoofing checks to ensure that egress traffic is not sent with arbitrary source IPs.\n- Firewall controls for Pods can be applied separately from their nodes.\n- Alias IPs allow Pods to directly access hosted services without using a NAT gateway.",
                    "impact": 1,
                    "title": "Ensure Kubernetes Cluster is created with Alias IP ranges enabled",
                    "source_location": {
                        "ref": "controls/kubernetes_7_13.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.14",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.14' do\n  impact 1.0\n  title 'Ensure PodSecurityPolicy controller is enabled on the Kubernetes Engine Clusters'\n  desc '\n  A Pod Security Policy is a cluster-level resource that controls security sensitive aspects of the pod\n  specification. The PodSecurityPolicy objects define a set of conditions that a pod must run with in order to\n  be accepted into the system, as well as defaults for the related fields.\n\n  Rationale:\n  The PodSecurityPolicy defines a set of conditions that Pods must meet to be accepted by the cluster; when a request\n  to create or update a Pod does not meet the conditions in the PodSecurityPolicy, that request is rejected and an\n  error is returned. The PodSecurityPolicy admission controller validates requests against available\n  PodSecurityPolicies. PodSecurityPolicies specify a list of restrictions, requirements, and defaults for Pods\n  created under the policy.\n  '\n  tag cis: 'gcp:7.14'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_pod_security_policy_config }\n      end\n    end\n  end\nend\n",
                    "desc": "A Pod Security Policy is a cluster-level resource that controls security sensitive aspects of the pod\nspecification. The PodSecurityPolicy objects define a set of conditions that a pod must run with in order to\nbe accepted into the system, as well as defaults for the related fields.\n\nRationale:\nThe PodSecurityPolicy defines a set of conditions that Pods must meet to be accepted by the cluster; when a request\nto create or update a Pod does not meet the conditions in the PodSecurityPolicy, that request is rejected and an\nerror is returned. The PodSecurityPolicy admission controller validates requests against available\nPodSecurityPolicies. PodSecurityPolicies specify a list of restrictions, requirements, and defaults for Pods\ncreated under the policy.",
                    "impact": 1,
                    "title": "Ensure PodSecurityPolicy controller is enabled on the Kubernetes Engine Clusters",
                    "source_location": {
                        "ref": "controls/kubernetes_7_14.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.15",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.15' do\n  impact 1.0\n  title 'Ensure Kubernetes Cluster is created with Private cluster enabled'\n  desc '\n  A private cluster is a cluster that makes your master inaccessible from the public internet. In a private cluster,\n  nodes do not have public IP addresses, so your workloads run in an environment that is isolated from the\n  internet. Nodes have addressed only in the private RFC 1918 address space. Nodes and masters communicate with each\n  other privately using VPC peering.\n\n  Rationale:\n  With a Private cluster enabled, VPC network peering gives you several advantages over using external IP addresses\n  or VPNs to connect networks, including:\n  - Network Latency: Public IP networking suffers higher latency than private networking.\n  - Network Security: Service owners do not need to have their services exposed to the public Internet and\n    deal with its associated risks.\n  - Network Cost: GCP charges egress bandwidth pricing for networks using external IPs to communicate even if the\n    traffic is within the same zone. If however, the networks are peered they can use internal IPs to communicate and\n    save on those egress costs. Regular network pricing still applies to all traffic.\n  '\n  tag cis: 'gcp:7.15'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should be_a_private_cluster }\n      end\n    end\n  end\nend\n",
                    "desc": "A private cluster is a cluster that makes your master inaccessible from the public internet. In a private cluster,\nnodes do not have public IP addresses, so your workloads run in an environment that is isolated from the\ninternet. Nodes have addressed only in the private RFC 1918 address space. Nodes and masters communicate with each\nother privately using VPC peering.\n\nRationale:\nWith a Private cluster enabled, VPC network peering gives you several advantages over using external IP addresses\nor VPNs to connect networks, including:\n- Network Latency: Public IP networking suffers higher latency than private networking.\n- Network Security: Service owners do not need to have their services exposed to the public Internet and\n  deal with its associated risks.\n- Network Cost: GCP charges egress bandwidth pricing for networks using external IPs to communicate even if the\n  traffic is within the same zone. If however, the networks are peered they can use internal IPs to communicate and\n  save on those egress costs. Regular network pricing still applies to all traffic.",
                    "impact": 1,
                    "title": "Ensure Kubernetes Cluster is created with Private cluster enabled",
                    "source_location": {
                        "ref": "controls/kubernetes_7_15.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.16",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.16' do\n  impact 1.0\n  title 'Ensure Private Google Access is set on Kubernetes Engine Cluster Subnets'\n  desc '\n  Private Google Access enables your cluster hosts, which have only private IP addresses, to communicate with Google\n  APIs and services using an internal IP address rather than an external IP address. External IP addresses are\n  routable and reachable over the Internet. Internal (private) IP addresses are internal to Google Cloud Platform\n  and are not routable or reachable over the Internet. You can use Private Google Access to allow VMs without Internet\n  access to reach Google APIs, services, and properties that are accessible over HTTP/HTTPS.\n\n  Rationale:\n  VPC networks and subnetworks provide logically isolated and secure network partitions where you can launch GCP\n  resources. When Private Google Access is enabled, VM instances in a subnet can reach the Google Cloud and Developer\n  APIs and services without needing an external IP address. Instead, VMs can use their internal IP addresses to access\n  Google managed services. Instances with external IP addresses are not affected when you enable the ability to access\n  Google services from internal IP addresses. These instances can still connect to Google APIs and managed services.\n  '\n  tag cis: 'gcp:7.16'\n  tag level: 1\n\n  google_compute_regions(project: gcp_project_id).region_names.each do |region_name|\n    google_compute_region(project: gcp_project_id, name: region_name).zone_names.each do |zone_name|\n      google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_subnetworks.each do |subnetwork_name|\n        describe google_compute_subnetwork(project: gcp_project_id, region: region_name, name: subnetwork_name) do\n          its('private_ip_google_access') { should be true }\n        end\n      end\n    end\n  end\nend\n",
                    "desc": "Private Google Access enables your cluster hosts, which have only private IP addresses, to communicate with Google\nAPIs and services using an internal IP address rather than an external IP address. External IP addresses are\nroutable and reachable over the Internet. Internal (private) IP addresses are internal to Google Cloud Platform\nand are not routable or reachable over the Internet. You can use Private Google Access to allow VMs without Internet\naccess to reach Google APIs, services, and properties that are accessible over HTTP/HTTPS.\n\nRationale:\nVPC networks and subnetworks provide logically isolated and secure network partitions where you can launch GCP\nresources. When Private Google Access is enabled, VM instances in a subnet can reach the Google Cloud and Developer\nAPIs and services without needing an external IP address. Instead, VMs can use their internal IP addresses to access\nGoogle managed services. Instances with external IP addresses are not affected when you enable the ability to access\nGoogle services from internal IP addresses. These instances can still connect to Google APIs and managed services.",
                    "impact": 1,
                    "title": "Ensure Private Google Access is set on Kubernetes Engine Cluster Subnets",
                    "source_location": {
                        "ref": "controls/kubernetes_7_16.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.17",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.17' do\n  only_if { cis_level == 2 }\n  impact 1.0\n  title 'Ensure default Service account is not used for Project access in Kubernetes Clusters'\n  desc '\n  A service account is an identity that an instance or an application can use to run API requests on your behalf. This\n  identity is used to identify applications running on your virtual machine instances to other Google Cloud Platform\n  services. By default, Kubernetes Engine nodes are given the Compute Engine default service account. This account\n  has broad access by default, making it useful to a wide variety of applications, but it has more permissions than\n  are required to run your Kubernetes Engine cluster.\n\n  Rationale:\n  You should create and use a minimally privileged service account to run your Kubernetes Engine cluster instead of\n  using the Compute Engine default service account. If you are not creating a separate service account for your\n  nodes, you should limit the scopes of the node service account to reduce the possibility of a privilege escalation\n  in an attack. Kubernetes Engine requires, at a minimum, the service account to have the monitoring.viewer,\n  monitoring.metricWriter, and logging.logWriter roles. This ensures that your default service account does not have\n  permissions beyond those necessary to run your cluster. While the default scopes are limited, they may include\n  scopes beyond the minimally required scopes needed to run your cluster.\n  '\n  tag cis: 'gcp:7.17'\n  tag level: 2\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      google_container_node_pools(project: gcp_project_id, zone: zone_name, cluster_name: cluster_name).node_pool_names.each do |node_pool_name|\n        describe google_container_node_pool(project: gcp_project_id, zone: zone_name, cluster_name: cluster_name, nodepool_name: node_pool_name) do\n          its('config_service_account') { should_not eq 'default' }\n        end\n      end\n    end\n  end\nend\n",
                    "desc": "A service account is an identity that an instance or an application can use to run API requests on your behalf. This\nidentity is used to identify applications running on your virtual machine instances to other Google Cloud Platform\nservices. By default, Kubernetes Engine nodes are given the Compute Engine default service account. This account\nhas broad access by default, making it useful to a wide variety of applications, but it has more permissions than\nare required to run your Kubernetes Engine cluster.\n\nRationale:\nYou should create and use a minimally privileged service account to run your Kubernetes Engine cluster instead of\nusing the Compute Engine default service account. If you are not creating a separate service account for your\nnodes, you should limit the scopes of the node service account to reduce the possibility of a privilege escalation\nin an attack. Kubernetes Engine requires, at a minimum, the service account to have the monitoring.viewer,\nmonitoring.metricWriter, and logging.logWriter roles. This ensures that your default service account does not have\npermissions beyond those necessary to run your cluster. While the default scopes are limited, they may include\nscopes beyond the minimally required scopes needed to run your cluster.",
                    "impact": 1,
                    "title": "Ensure default Service account is not used for Project access in Kubernetes Clusters",
                    "source_location": {
                        "ref": "controls/kubernetes_7_17.rb",
                        "line": 8
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.2",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.2' do\n  impact 1.0\n  title 'Ensure Stackdriver Monitoring is set to Enabled on Kubernetes Engine Clusters'\n  desc '\n  Stackdriver Monitoring to monitor signals and build operations in your Kubernetes Engine clusters. Stackdriver\n  Monitoring can access metrics about CPU utilization, some disk traffic metrics, network traffic, and uptime\n  information. Stackdriver Monitoring uses the Monitoring agent to access additional system resources and application\n  services in virtual machine instances.\n\n  Rationale:\n  By Enabling Stackdriver Monitoring you will have system metrics and custom metrics. System metrics are\n  measurements of the cluster\\'s infrastructure, such as CPU or memory usage. For system metrics, Stackdriver\n  creates a Deployment that periodically connects to each node and collects metrics about its Pods and containers,\n  then sends the metrics to Stackdriver. Metrics for usage of system resources are collected from the CPU, Memory,\n  Evictable memory, Non-evictable memory, and Disk sources.\n  '\n  tag cis: 'gcp:7.2'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_monitoring_enabled }\n      end\n    end\n  end\nend\n",
                    "desc": "Stackdriver Monitoring to monitor signals and build operations in your Kubernetes Engine clusters. Stackdriver\nMonitoring can access metrics about CPU utilization, some disk traffic metrics, network traffic, and uptime\ninformation. Stackdriver Monitoring uses the Monitoring agent to access additional system resources and application\nservices in virtual machine instances.\n\nRationale:\nBy Enabling Stackdriver Monitoring you will have system metrics and custom metrics. System metrics are\nmeasurements of the cluster's infrastructure, such as CPU or memory usage. For system metrics, Stackdriver\ncreates a Deployment that periodically connects to each node and collects metrics about its Pods and containers,\nthen sends the metrics to Stackdriver. Metrics for usage of system resources are collected from the CPU, Memory,\nEvictable memory, Non-evictable memory, and Disk sources.",
                    "impact": 1,
                    "title": "Ensure Stackdriver Monitoring is set to Enabled on Kubernetes Engine Clusters",
                    "source_location": {
                        "ref": "controls/kubernetes_7_2.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.3",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.3' do\n  impact 1.0\n  title 'Ensure Legacy Authorization is set to Disabled on Kubernetes Engine Clusters'\n  desc '\n  In Kubernetes, authorizers interact by granting a permission if any authorizer grants the permission. The\n  legacy authorizer in Kubernetes Engine grants broad, statically defined permissions. To ensure that RBAC limits\n  permissions correctly, you must disable the legacy authorizer. RBAC has significant security advantages, can\n  help you ensure that users only have access to cluster resources within their own namespace and is now\n  stable in Kubernetes.\n\n  Rationale:\n  Enable Legacy Authorization for in-cluster permissions that support existing clusters or workflows. Disable\n  legacy authorization for full RBAC support for in-cluster permissions. In Kubernetes, RBAC is used to grant\n  permissions to resources at the cluster and namespace level. RBAC allows you to define roles with rules\n  containing a set of permissions.\n  '\n  tag cis: 'gcp:7.3'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_legacy_abac_disabled }\n      end\n    end\n  end\nend\n",
                    "desc": "In Kubernetes, authorizers interact by granting a permission if any authorizer grants the permission. The\nlegacy authorizer in Kubernetes Engine grants broad, statically defined permissions. To ensure that RBAC limits\npermissions correctly, you must disable the legacy authorizer. RBAC has significant security advantages, can\nhelp you ensure that users only have access to cluster resources within their own namespace and is now\nstable in Kubernetes.\n\nRationale:\nEnable Legacy Authorization for in-cluster permissions that support existing clusters or workflows. Disable\nlegacy authorization for full RBAC support for in-cluster permissions. In Kubernetes, RBAC is used to grant\npermissions to resources at the cluster and namespace level. RBAC allows you to define roles with rules\ncontaining a set of permissions.",
                    "impact": 1,
                    "title": "Ensure Legacy Authorization is set to Disabled on Kubernetes Engine Clusters",
                    "source_location": {
                        "ref": "controls/kubernetes_7_3.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.4",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.4' do\n  impact 1.0\n  title 'Ensure Master authorized networks is set to Enabled on Kubernetes Engine Clusters'\n  desc '\n  Authorized networks are a way of specifying a restricted range of IP addresses that are permitted to access your\n  container cluster\\'s Kubernetes master endpoint. Kubernetes Engine uses both Transport Layer Security (TLS) and\n  authentication to provide secure access to your container cluster\\'s Kubernetes master endpoint from the public\n  internet. This provides you the flexibility to administer your cluster from anywhere; however, you might want to\n  further restrict access to a set of IP addresses that you control. You can set this restriction by specifying an\n  authorized network.\n\n  Rationale:\n  By Enabling, Master authorized networks blocks untrusted IP addresses from outside Google Cloud Platform and\n  Addresses from inside GCP (such as traffic from Compute Engine VMs) can reach your master through HTTPS provided\n  that they have the necessary Kubernetes credentials.\n\n  Restricting access to an authorized network can provide additional security benefits for your container\n  cluster, including:\n  - Better Protection from Outsider Attacks: Authorized networks provide an additional layer of security by\n    limiting external, non-GCP access to a specific set of addresses you designate, such as those that originate\n    from your premises. This helps protect access to your cluster in the case of a vulnerability in the\n    cluster\\'s authentication or authorization mechanism.\n  - Better Protection from Insider Attacks: Authorized networks help protect your cluster from accidental leaks of\n    master certificates from your company\\'s premises. Leaked certificates used from outside GCP and outside the\n    authorized IP ranges--for example, from addresses outside your company--are still denied access.\n  '\n  tag cis: 'gcp:7.4'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_master_authorized_networks_enabled }\n      end\n    end\n  end\nend\n",
                    "desc": "Authorized networks are a way of specifying a restricted range of IP addresses that are permitted to access your\ncontainer cluster's Kubernetes master endpoint. Kubernetes Engine uses both Transport Layer Security (TLS) and\nauthentication to provide secure access to your container cluster's Kubernetes master endpoint from the public\ninternet. This provides you the flexibility to administer your cluster from anywhere; however, you might want to\nfurther restrict access to a set of IP addresses that you control. You can set this restriction by specifying an\nauthorized network.\n\nRationale:\nBy Enabling, Master authorized networks blocks untrusted IP addresses from outside Google Cloud Platform and\nAddresses from inside GCP (such as traffic from Compute Engine VMs) can reach your master through HTTPS provided\nthat they have the necessary Kubernetes credentials.\n\nRestricting access to an authorized network can provide additional security benefits for your container\ncluster, including:\n- Better Protection from Outsider Attacks: Authorized networks provide an additional layer of security by\n  limiting external, non-GCP access to a specific set of addresses you designate, such as those that originate\n  from your premises. This helps protect access to your cluster in the case of a vulnerability in the\n  cluster's authentication or authorization mechanism.\n- Better Protection from Insider Attacks: Authorized networks help protect your cluster from accidental leaks of\n  master certificates from your company's premises. Leaked certificates used from outside GCP and outside the\n  authorized IP ranges--for example, from addresses outside your company--are still denied access.",
                    "impact": 1,
                    "title": "Ensure Master authorized networks is set to Enabled on Kubernetes Engine Clusters",
                    "source_location": {
                        "ref": "controls/kubernetes_7_4.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.5",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.5' do\n  impact 1.0\n  title 'Ensure Kubernetes Clusters are configured with Labels'\n  desc '\n  A cluster label is a key-value pair that helps you organize your Google Cloud Platform resources, such as\n  clusters. You can attach a label to each resource, then filter the resources based on their labels. Information\n  about labels is forwarded to the billing system, so you can break down your billing charges by the label.\n\n  Rationale:\n  Configured Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at\n  creation time and subsequently added and modified at any time. Each object can have a set of key/value labels\n  defined. Each Key must be unique for a given object. Labels enable users to map their own organizational structures\n  onto system objects in a loosely coupled fashion, without requiring clients to store these mappings. Labels can also\n  be used to apply specific security settings and \\'auto configure\\' objects at creation.\n  '\n  tag cis: 'gcp:7.5'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_resource_labels }\n      end\n    end\n  end\nend\n",
                    "desc": "A cluster label is a key-value pair that helps you organize your Google Cloud Platform resources, such as\nclusters. You can attach a label to each resource, then filter the resources based on their labels. Information\nabout labels is forwarded to the billing system, so you can break down your billing charges by the label.\n\nRationale:\nConfigured Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at\ncreation time and subsequently added and modified at any time. Each object can have a set of key/value labels\ndefined. Each Key must be unique for a given object. Labels enable users to map their own organizational structures\nonto system objects in a loosely coupled fashion, without requiring clients to store these mappings. Labels can also\nbe used to apply specific security settings and 'auto configure' objects at creation.",
                    "impact": 1,
                    "title": "Ensure Kubernetes Clusters are configured with Labels",
                    "source_location": {
                        "ref": "controls/kubernetes_7_5.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.6",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.6' do\n  impact 1.0\n  title 'Ensure Kubernetes web UI / Dashboard is disabled'\n  desc '\n  A cluster label is a key-value pair that helps you organize your Google Cloud Platform resources, such as\n  clusters. You can attach a label to each resource, then filter the resources based on their labels. Information\n  about labels is forwarded to the billing system, so you can break down your billing charges by the label.\n\n  Rationale:\n  Configured Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at\n  creation time and subsequently added and modified at any time. Each object can have a set of key/value labels\n  defined. Each Key must be unique for a given object. Labels enable users to map their own organizational structures\n  onto system objects in a loosely coupled fashion, without requiring clients to store these mappings. Labels can also\n  be used to apply specific security settings and \\'auto configure\\' objects at creation.\n  '\n  tag cis: 'gcp:7.6'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      describe google_container_cluster(project: gcp_project_id, zone: zone_name, name: cluster_name) do\n        it { should have_kubernetes_dashboard_disabled }\n      end\n    end\n  end\nend\n",
                    "desc": "A cluster label is a key-value pair that helps you organize your Google Cloud Platform resources, such as\nclusters. You can attach a label to each resource, then filter the resources based on their labels. Information\nabout labels is forwarded to the billing system, so you can break down your billing charges by the label.\n\nRationale:\nConfigured Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at\ncreation time and subsequently added and modified at any time. Each object can have a set of key/value labels\ndefined. Each Key must be unique for a given object. Labels enable users to map their own organizational structures\nonto system objects in a loosely coupled fashion, without requiring clients to store these mappings. Labels can also\nbe used to apply specific security settings and 'auto configure' objects at creation.",
                    "impact": 1,
                    "title": "Ensure Kubernetes web UI / Dashboard is disabled",
                    "source_location": {
                        "ref": "controls/kubernetes_7_6.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.7",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.7' do\n  impact 1.0\n  title 'Ensure `Automatic node repair` is enabled for Kubernetes Clusters'\n  desc '\n  Kubernetes Engine\\'s node auto-repair feature helps you keep the nodes in your cluster in a healthy, running\n  state. When enabled, Kubernetes Engine makes periodic checks on the health state of each node in your cluster. If\n  a node fails consecutive health checks over an extended time period, Kubernetes Engine initiates a repair process\n  for that node. If you disable node auto-repair at any time during the repair process, the in-progress repairs are\n  not cancelled and still complete for any node currently under repair.\n\n  Rationale:\n  Kubernetes Engine uses the node\\'s health status to determine if a node needs to be repaired. A node reporting\n  a Ready status is considered healthy. Kubernetes Engine triggers a repair action if a node reports consecutive\n  unhealthy status reports for a given time threshold. An unhealthy status can mean:                                                                                                                                                                                                                                                                                                                                         A node reports a NotReady status on consecutive checks over the given time threshold (approximately 10 minutes).\n  - A node does not report any status at all over the given time threshold (approximately 10 minutes).\n  - A node\\'s boot disk is out of disk space for an extended time period (approximately 30 minutes).\n\n  You can enable node auto-repair on a per-node pool basis. When you create a cluster, you can enable or disable\n  auto-repair for the cluster\\'s default node pool. If you create additional node pools, you can enable or disable\n  node auto-repair for those node pools, independent of the auto-repair setting for the default node pool.\n  Kubernetes Engine generates an entry in its operation logs for any automated repair event. You can check the\n  logs by using the gcloud container operations list command.\n  '\n  tag cis: 'gcp:7.7'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      google_container_node_pools(project: gcp_project_id, zone: zone_name, cluster_name: cluster_name).node_pool_names.each do |node_pool_name|\n        describe google_container_node_pool(project: gcp_project_id, zone: zone_name, cluster_name: cluster_name, nodepool_name: node_pool_name) do\n          it { should have_automatic_node_repair }\n        end\n      end\n    end\n  end\nend\n",
                    "desc": "Kubernetes Engine's node auto-repair feature helps you keep the nodes in your cluster in a healthy, running\nstate. When enabled, Kubernetes Engine makes periodic checks on the health state of each node in your cluster. If\na node fails consecutive health checks over an extended time period, Kubernetes Engine initiates a repair process\nfor that node. If you disable node auto-repair at any time during the repair process, the in-progress repairs are\nnot cancelled and still complete for any node currently under repair.\n\nRationale:\nKubernetes Engine uses the node's health status to determine if a node needs to be repaired. A node reporting\na Ready status is considered healthy. Kubernetes Engine triggers a repair action if a node reports consecutive\nunhealthy status reports for a given time threshold. An unhealthy status can mean:                                                                                                                                                                                                                                                                                                                                         A node reports a NotReady status on consecutive checks over the given time threshold (approximately 10 minutes).\n- A node does not report any status at all over the given time threshold (approximately 10 minutes).\n- A node's boot disk is out of disk space for an extended time period (approximately 30 minutes).\n\nYou can enable node auto-repair on a per-node pool basis. When you create a cluster, you can enable or disable\nauto-repair for the cluster's default node pool. If you create additional node pools, you can enable or disable\nnode auto-repair for those node pools, independent of the auto-repair setting for the default node pool.\nKubernetes Engine generates an entry in its operation logs for any automated repair event. You can check the\nlogs by using the gcloud container operations list command.",
                    "impact": 1,
                    "title": "Ensure `Automatic node repair` is enabled for Kubernetes Clusters",
                    "source_location": {
                        "ref": "controls/kubernetes_7_7.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.8",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.8' do\n  impact 1.0\n  title 'Ensure Automatic node upgrades is enabled on Kubernetes Engine Clusters nodes'\n  desc '\n  Node auto-upgrades help you keep the nodes in your cluster or node pool up to date with the latest stable version\n  of Kubernetes. Auto-Upgrades use the same update mechanism as manual node upgrades.\n\n  Rationale:\n  Node pools with auto-upgrades enabled are automatically scheduled for upgrades when a new stable Kubernetes\n  version becomes available. When the upgrade is performed, the node pool is upgraded to match the current cluster\n  master version. Some benefits of using enabling auto-upgrades are:\n  - Lower management overhead: You don\\'t have to manually track and update to the latest version of Kubernetes.\n  - Better security: Sometimes new binaries are released to fix a security issue. With auto-upgrades, Kubernetes\n    Engine automatically ensures that security updates are applied and kept up to date.\n  - Ease of use: Provides a simple way to keep your nodes up to date with the latest Kubernetes features.\n  '\n  tag cis: 'gcp:7.8'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      google_container_node_pools(project: gcp_project_id, zone: zone_name, cluster_name: cluster_name).node_pool_names.each do |node_pool_name|\n        describe google_container_node_pool(project: gcp_project_id, zone: zone_name, cluster_name: cluster_name, nodepool_name: node_pool_name) do\n          it { should have_automatic_node_upgrade }\n        end\n      end\n    end\n  end\nend\n",
                    "desc": "Node auto-upgrades help you keep the nodes in your cluster or node pool up to date with the latest stable version\nof Kubernetes. Auto-Upgrades use the same update mechanism as manual node upgrades.\n\nRationale:\nNode pools with auto-upgrades enabled are automatically scheduled for upgrades when a new stable Kubernetes\nversion becomes available. When the upgrade is performed, the node pool is upgraded to match the current cluster\nmaster version. Some benefits of using enabling auto-upgrades are:\n- Lower management overhead: You don't have to manually track and update to the latest version of Kubernetes.\n- Better security: Sometimes new binaries are released to fix a security issue. With auto-upgrades, Kubernetes\n  Engine automatically ensures that security updates are applied and kept up to date.\n- Ease of use: Provides a simple way to keep your nodes up to date with the latest Kubernetes features.",
                    "impact": 1,
                    "title": "Ensure Automatic node upgrades is enabled on Kubernetes Engine Clusters nodes",
                    "source_location": {
                        "ref": "controls/kubernetes_7_8.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-kubernetes-7.9",
                    "code": "control 'cis-gcp-benchmark-kubernetes-7.9' do\n  only_if { cis_level == 2 }\n  impact 1.0\n  title 'Ensure Automatic node upgrades is enabled on Kubernetes Engine Clusters nodes'\n  desc '\n  Container-Optimized OS is an operating system image for your Compute Engine VMs that is optimized for running Docker\n  containers. With Container-Optimized OS, you can bring up your Docker containers on Google Cloud Platform quickly,\n  efficiently, and securely.\n\n  Rationale:\n  The Container-Optimized OS node image is based on a recent version of the Linux kernel and is optimized to enhance\n  node security. It is backed by a team at Google that can quickly patch it for security and iterate on features. The\n  Container-Optimized OS image provides better support, security, and stability than previous images.\n  Container-Optimized OS requires Kubernetes version 1.4.0 or higher.\n\n  Enabling Container-Optimized OS provides the following benefits:\n  - Run Containers Out of the Box: Container-Optimized OS instances come pre- installed with the Docker runtime and cloud-init. With a Container-Optimized OS instance, you can bring up your Docker container at the same time you create your VM, with no on-host setup required.\n  - Smaller attack surface: Container-Optimized OS has a smaller footprint, reducing your instance\\'s potential\n    attack surface.\n  - Locked-down by default: Container-Optimized OS instances include a locked-down firewall and other security\n    settings by default.\n  - Automatic Updates: Container-Optimized OS instances are configured to automatically download weekly updates in the\n    background; only a reboot is necessary to use the latest updates.\n  '\n  tag cis: 'gcp:7.9'\n  tag level: 2\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_container_clusters(project: gcp_project_id, zone: zone_name).cluster_names.each do |cluster_name|\n      google_container_node_pools(project: gcp_project_id, zone: zone_name, cluster_name: cluster_name).node_pool_names.each do |node_pool_name|\n        describe google_container_node_pool(project: gcp_project_id, zone: zone_name, cluster_name: cluster_name, nodepool_name: node_pool_name) do\n          its('config_image_type') { should eq 'COS' }\n        end\n      end\n    end\n  end\nend\n",
                    "desc": "Container-Optimized OS is an operating system image for your Compute Engine VMs that is optimized for running Docker\ncontainers. With Container-Optimized OS, you can bring up your Docker containers on Google Cloud Platform quickly,\nefficiently, and securely.\n\nRationale:\nThe Container-Optimized OS node image is based on a recent version of the Linux kernel and is optimized to enhance\nnode security. It is backed by a team at Google that can quickly patch it for security and iterate on features. The\nContainer-Optimized OS image provides better support, security, and stability than previous images.\nContainer-Optimized OS requires Kubernetes version 1.4.0 or higher.\n\nEnabling Container-Optimized OS provides the following benefits:\n- Run Containers Out of the Box: Container-Optimized OS instances come pre- installed with the Docker runtime and cloud-init. With a Container-Optimized OS instance, you can bring up your Docker container at the same time you create your VM, with no on-host setup required.\n- Smaller attack surface: Container-Optimized OS has a smaller footprint, reducing your instance's potential\n  attack surface.\n- Locked-down by default: Container-Optimized OS instances include a locked-down firewall and other security\n  settings by default.\n- Automatic Updates: Container-Optimized OS instances are configured to automatically download weekly updates in the\n  background; only a reboot is necessary to use the latest updates.",
                    "impact": 1,
                    "title": "Ensure Automatic node upgrades is enabled on Kubernetes Engine Clusters nodes",
                    "source_location": {
                        "ref": "controls/kubernetes_7_9.rb",
                        "line": 8
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.1",
                    "code": "control 'cis-gcp-benchmark-logging-2.1' do\n  impact 1.0\n  title 'Ensure that Cloud Audit Logging is configured properly across all services and all users from a project'\n  desc '\n  It is recommended that Cloud Audit Logging is configured to track all Admin activities and read, write access to\n  user data.\n\n  Rationale:\n  Cloud Audit Logging maintains two audit logs for each project and organization: Admin Activity and Data Access.\n  1. Admin Activity logs contain log entries for API calls or other administrative actions that modify the\n  configuration or metadata of resources. Admin Activity audit logs are enabled for all services and cannot be\n  configured.\n  2. Data Access audit logs record API calls that create, modify, or read user-provided data. These are disabled by\n  default and should be enabled.\n  There are three kinds of Data Access audit log information:\n  * Admin read: Records operations that read metadata or configuration information. Admin Activity audit logs record\n  writes of metadata and configuration information which cannot be disabled.\n  * Data read: Records operations that read user-provided data. o Data write: Records operations that write\n  user-provided data.\n  It is recommended to have effective default audit config configured in such a way that:\n  1. logtype is set to DATA_READ (to logs user activity tracking) and DATA_WRITES (to log changes/tampering to\n  user data)\n  2. audit config is enabled for all the services supported by Data Access audit logs feature\n  3. Logs should be captured for all users i.e.. there are no exempted users in any of the audit config section. This\n  will ensure overriding audit config will not contradict the requirement.\n  '\n  tag cis: 'gcp:2.1'\n  tag level: 1\n\n  describe google_project_logging_audit_config(project: gcp_project_id) do\n    its('default_types') { should include 'DATA_WRITE' }\n    its('default_types') { should include 'DATA_READ' }\n    it { should_not have_default_exempted_members }\n  end\n\nend\n",
                    "desc": "It is recommended that Cloud Audit Logging is configured to track all Admin activities and read, write access to\nuser data.\n\nRationale:\nCloud Audit Logging maintains two audit logs for each project and organization: Admin Activity and Data Access.\n1. Admin Activity logs contain log entries for API calls or other administrative actions that modify the\nconfiguration or metadata of resources. Admin Activity audit logs are enabled for all services and cannot be\nconfigured.\n2. Data Access audit logs record API calls that create, modify, or read user-provided data. These are disabled by\ndefault and should be enabled.\nThere are three kinds of Data Access audit log information:\n* Admin read: Records operations that read metadata or configuration information. Admin Activity audit logs record\nwrites of metadata and configuration information which cannot be disabled.\n* Data read: Records operations that read user-provided data. o Data write: Records operations that write\nuser-provided data.\nIt is recommended to have effective default audit config configured in such a way that:\n1. logtype is set to DATA_READ (to logs user activity tracking) and DATA_WRITES (to log changes/tampering to\nuser data)\n2. audit config is enabled for all the services supported by Data Access audit logs feature\n3. Logs should be captured for all users i.e.. there are no exempted users in any of the audit config section. This\nwill ensure overriding audit config will not contradict the requirement.",
                    "impact": 1,
                    "title": "Ensure that Cloud Audit Logging is configured properly across all services and all users from a project",
                    "source_location": {
                        "ref": "controls/logging_2_1.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Logging Audit Config For dmurray-project ",
                            "run_time": 0.000042452,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.10",
                    "code": "control 'cis-gcp-benchmark-logging-2.10' do\n  impact 1.0\n  title 'Ensure log metric filter and alerts exists for VPC network changes'\n  desc '\n  It is recommended that a metric filter and alarm be established for Cloud Storage Bucket IAM changes.\n\n  Rationale:\n  Monitoring changes to Cloud Storage bucket permissions may reduce time to detect and correct permissions on\n  sensitive Cloud Storage bucket and objects inside the bucket.\n  '\n  tag cis: 'gcp:2.10'\n  tag level: 1\n\n  filter = \"resource.type=gcs_bucket AND protoPayload.methodName=\\\"storage.setIamPermissions\\\"\\n\"\n  describe google_project_metrics(project: gcp_project_id).where(metric_filter: filter) do\n    it { should exist }\n  end\n\n  alert_policy_exists = false\n  google_project_metrics(project: gcp_project_id).where(metric_filter: filter).metric_types.each do |metric_type|\n    metric_filter = \"metric.type=\\\"#{metric_type}\\\" project=\\\"#{gcp_project_id}\\\"\"\n    google_project_alert_policies(project: gcp_project_id).where(policy_enabled_state: true).where { policy_filter_list.include?(metric_filter) }.policy_names.each do |policy_name|\n      describe google_project_alert_policy_condition(policy: policy_name, filter: metric_filter) do\n        alert_policy_exists=true\n        it { should exist }\n        its('condition_threshold_value') { should eq 0.001 }\n        its('aggregation_alignment_period') { should eq '60s' }\n        its('aggregation_cross_series_reducer') { should eq 'REDUCE_COUNT' }\n        its('aggregation_per_series_aligner') { should eq 'ALIGN_RATE' }\n      end\n    end\n  end\n\n  describe alert_policy_exists do\n    it \"Alert policy for filter \\\"#{filter}\\\" does not exist\" do\n      expect(alert_policy_exists).to(be true)\n    end\n  end\nend\n",
                    "desc": "It is recommended that a metric filter and alarm be established for Cloud Storage Bucket IAM changes.\n\nRationale:\nMonitoring changes to Cloud Storage bucket permissions may reduce time to detect and correct permissions on\nsensitive Cloud Storage bucket and objects inside the bucket.",
                    "impact": 1,
                    "title": "Ensure log metric filter and alerts exists for VPC network changes",
                    "source_location": {
                        "ref": "controls/logging_2_10.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_project_metrics with metric_filter == \"resource.type=gcs_bucket AND protoPayload.methodName=\\\"storage.setIamPermissions\\\"\\n\" ",
                            "run_time": 0.000051691,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "false Alert policy for filter \"resource.type=gcs_bucket AND protoPayload.methodName=\"storage.setIamPermissions\"\n\" does not exist",
                            "run_time": 0.000058966,
                            "start_time": "",
                            "message": "undefined method `expect' for #<RSpec::ExampleGroups::False:0x000000000beaded0>",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.11",
                    "code": "control 'cis-gcp-benchmark-logging-2.11' do\n  impact 1.0\n  title 'Ensure log metric filter and alerts exists for SQL instance configuration changes'\n  desc '\n  It is recommended that a metric filter and alarm be established for SQL Instance configuration changes.\n\n  Rationale:\n  Monitoring changes to Sql Instance configuration changes may reduce time to detect and correct\n  misconfigurations done on sql server.\n  Below are the few of configurable Options which may impact security posture of a SQL Instance:\n   - Enable auto backups and high availability: Misconfiguration may adversely impact Business continuity,\n     Disaster Recovery and High Availability\n   - Authorize networks : Misconfiguration may increase exposure to the untrusted networks\n  '\n  tag cis: 'gcp:2.11'\n  tag level: 1\n\n  filter = \"protoPayload.methodName=\\\"cloudsql.instances.update\\\"\\n\"\n  describe google_project_metrics(project: gcp_project_id).where(metric_filter: filter) do\n    it { should exist }\n  end\n\n  alert_policy_exists = false\n  google_project_metrics(project: gcp_project_id).where(metric_filter: filter).metric_types.each do |metric_type|\n    metric_filter = \"metric.type=\\\"#{metric_type}\\\" project=\\\"#{gcp_project_id}\\\"\"\n    google_project_alert_policies(project: gcp_project_id).where(policy_enabled_state: true).where { policy_filter_list.include?(metric_filter) }.policy_names.each do |policy_name|\n      describe google_project_alert_policy_condition(policy: policy_name, filter: metric_filter) do\n        alert_policy_exists=true\n        it { should exist }\n        its('condition_threshold_value') { should eq 0.001 }\n        its('aggregation_alignment_period') { should eq '60s' }\n        its('aggregation_cross_series_reducer') { should eq 'REDUCE_COUNT' }\n        its('aggregation_per_series_aligner') { should eq 'ALIGN_RATE' }\n      end\n    end\n  end\n\n  describe alert_policy_exists do\n    it \"Alert policy for filter \\\"#{filter}\\\" does not exist\" do\n      expect(alert_policy_exists).to(be true)\n    end\n  end\nend\n",
                    "desc": "It is recommended that a metric filter and alarm be established for SQL Instance configuration changes.\n\nRationale:\nMonitoring changes to Sql Instance configuration changes may reduce time to detect and correct\nmisconfigurations done on sql server.\nBelow are the few of configurable Options which may impact security posture of a SQL Instance:\n - Enable auto backups and high availability: Misconfiguration may adversely impact Business continuity,\n   Disaster Recovery and High Availability\n - Authorize networks : Misconfiguration may increase exposure to the untrusted networks",
                    "impact": 1,
                    "title": "Ensure log metric filter and alerts exists for SQL instance configuration changes",
                    "source_location": {
                        "ref": "controls/logging_2_11.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_project_metrics with metric_filter == \"protoPayload.methodName=\\\"cloudsql.instances.update\\\"\\n\" ",
                            "run_time": 0.00009861,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "false Alert policy for filter \"protoPayload.methodName=\"cloudsql.instances.update\"\n\" does not exist",
                            "run_time": 0.00004681,
                            "start_time": "",
                            "message": "undefined method `expect' for #<RSpec::ExampleGroups::False_2:0x000000000bea2f08>",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.2",
                    "code": "control 'cis-gcp-benchmark-logging-2.2' do\n  impact 1.0\n  title 'Ensure that sinks are configured for all Log entries'\n  desc '\n  It is recommended to create sink which will export copies of all the log entries.\n\n  Rationale:\n  Log entries are held in Stackdriver Logging for a limited time known as the retention period. After that, the\n  entries are deleted. To keep log entries longer, sink can export them outside of Stackdriver Logging. Exporting\n  involves writing a filter that selects the log entries to export, and choosing a destination in Cloud Storage,\n  BigQuery, or Cloud Pub/Sub. The filter and destination are held in an object called a sink. To ensure all\n  log entries are exported using sink ensure that there is no filter configured for a sink. Sinks can be created\n  in projects, organizations, folders, and billing accounts.\n  '\n  tag cis: 'gcp:2.2'\n  tag level: 1\n\n  # Note this control is written for project level sinks\n  describe google_logging_project_sinks(project: gcp_project_id).where(sink_filter: nil) do\n    it { should exist }\n  end\n\n  describe google_logging_project_sinks(project: gcp_project_id).where(sink_filter: nil).where(sink_destination: nil) do\n    it { should_not exist }\n  end\n\nend\n",
                    "desc": "It is recommended to create sink which will export copies of all the log entries.\n\nRationale:\nLog entries are held in Stackdriver Logging for a limited time known as the retention period. After that, the\nentries are deleted. To keep log entries longer, sink can export them outside of Stackdriver Logging. Exporting\ninvolves writing a filter that selects the log entries to export, and choosing a destination in Cloud Storage,\nBigQuery, or Cloud Pub/Sub. The filter and destination are held in an object called a sink. To ensure all\nlog entries are exported using sink ensure that there is no filter configured for a sink. Sinks can be created\nin projects, organizations, folders, and billing accounts.",
                    "impact": 1,
                    "title": "Ensure that sinks are configured for all Log entries",
                    "source_location": {
                        "ref": "controls/logging_2_2.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_logging_project_sinks with sink_filter == nil ",
                            "run_time": 0.000040756,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_logging_project_sinks with sink_filter == nil sink_destination == nil ",
                            "run_time": 0.000043252,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.3",
                    "code": "control 'cis-gcp-benchmark-logging-2.3' do\n  impact 1.0\n  title 'Ensure that object versioning is enabled on log-buckets'\n  desc '\n  It is recommended to enable object versioning on log-buckets.\n\n  Rationale:\n  Logs can be exported by creating one or more sinks that include a logs filter and a destination. As Stackdriver\n  Logging receives new log entries, they are compared against each sink. If a log entry matches a sink\\'s filter,\n  then a copy of the log entry is written to the destination.\n\n  Sinks can be configured to export logs in Storage buckets. To support the retrieval of objects that are deleted or\n  overwritten, Object Versioning feature should be enabled on all such storage buckets where sinks are configured.\n  '\n  tag cis: 'gcp:2.3'\n  tag level: 1\n\n  google_logging_project_sinks(project: gcp_project_id).where(sink_destination: /storage.googleapis.com/).sink_destinations.each do |bucket|\n    describe google_storage_bucket(name: bucket.split('/').last) do\n      it { should have_versioning_enabled }\n    end\n  end\nend\n",
                    "desc": "It is recommended to enable object versioning on log-buckets.\n\nRationale:\nLogs can be exported by creating one or more sinks that include a logs filter and a destination. As Stackdriver\nLogging receives new log entries, they are compared against each sink. If a log entry matches a sink's filter,\nthen a copy of the log entry is written to the destination.\n\nSinks can be configured to export logs in Storage buckets. To support the retrieval of objects that are deleted or\noverwritten, Object Versioning feature should be enabled on all such storage buckets where sinks are configured.",
                    "impact": 1,
                    "title": "Ensure that object versioning is enabled on log-buckets",
                    "source_location": {
                        "ref": "controls/logging_2_3.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.4",
                    "code": "control 'cis-gcp-benchmark-logging-2.4' do\n  impact 1.0\n  title 'Ensure log metric filter and alerts exists for Project Ownership assignments/changes'\n  desc '\n  In order to prevent unnecessarily project ownership assignments to users/service- accounts and further misuses\n  of project and resources, all roles/Owner assignments should be monitored.\n  Members (users/Service-Accounts) with role assignment to primitive role roles/owner are Project Owners.\n\n  Project Owner has all the privileges on a project it belongs to. These can be summarized as below:\n  - All viewer permissions on All GCP Services part within the project\n  - Permissions for actions that modify state of All GCP Services within the project\n  - Manage roles and permissions for a project and all resources within the project\n  - Set up billing for a project\n\n  Granting owner role to a member (user/Service-Account) will allow members to modify the IAM policy. Therefore grant\n  the owner role only if the member has a legitimate purpose to manage the IAM policy. This is because as project IAM\n  policy contains sensitive access control data and having a minimal set of users manage it will simplify any auditing\n  that you may have to do.\n\n  Rationale:\n  Project Ownership Having highest level of privileges on a project, to avoid misuse of project resources project\n  ownership assignment/change actions mentioned should be monitored and alerted to concerned recipients.\n  - Sending project ownership Invites\n  - Acceptance/Rejection of project ownership invite by user\n  - Adding `role\\owner` to a user/service-account\n  - Removing a user/Service account from `role\\owner`\n  '\n  tag cis: 'gcp:2.4'\n  tag level: 1\n\n  filter = \"(protoPayload.serviceName=\\\"cloudresourcemanager.googleapis.com\\\") AND (ProjectOwnership OR projectOwnerInvitee) OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=\\\"REMOVE\\\" AND\\nprotoPayload.serviceData.policyDelta.bindingDeltas.role=\\\"roles/owner\\\") OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=\\\"ADD\\\" AND protoPayload.serviceData.policyDelta.bindingDeltas.role=\\\"roles/owner\\\")\\n\"\n  describe google_project_metrics(project: gcp_project_id).where(metric_filter: filter) do\n    it { should exist }\n  end\n\n  alert_policy_exists = false\n  google_project_metrics(project: gcp_project_id).where(metric_filter: filter).metric_types.each do |metric_type|\n    metric_filter = \"metric.type=\\\"#{metric_type}\\\" project=\\\"#{gcp_project_id}\\\"\"\n    google_project_alert_policies(project: gcp_project_id).where(policy_enabled_state: true).where { policy_filter_list.include?(metric_filter) }.policy_names.each do |policy_name|\n      describe google_project_alert_policy_condition(policy: policy_name, filter: metric_filter) do\n        alert_policy_exists=true\n        it { should exist }\n        its('condition_threshold_value') { should eq 0.001 }\n        its('aggregation_alignment_period') { should eq '60s' }\n        its('aggregation_cross_series_reducer') { should eq 'REDUCE_COUNT' }\n        its('aggregation_per_series_aligner') { should eq 'ALIGN_RATE' }\n      end\n    end\n  end\n\n  describe alert_policy_exists do\n    it \"Alert policy for filter \\\"#{filter}\\\" does not exist\" do\n      expect(alert_policy_exists).to(be true)\n    end\n  end\nend\n",
                    "desc": "In order to prevent unnecessarily project ownership assignments to users/service- accounts and further misuses\nof project and resources, all roles/Owner assignments should be monitored.\nMembers (users/Service-Accounts) with role assignment to primitive role roles/owner are Project Owners.\n\nProject Owner has all the privileges on a project it belongs to. These can be summarized as below:\n- All viewer permissions on All GCP Services part within the project\n- Permissions for actions that modify state of All GCP Services within the project\n- Manage roles and permissions for a project and all resources within the project\n- Set up billing for a project\n\nGranting owner role to a member (user/Service-Account) will allow members to modify the IAM policy. Therefore grant\nthe owner role only if the member has a legitimate purpose to manage the IAM policy. This is because as project IAM\npolicy contains sensitive access control data and having a minimal set of users manage it will simplify any auditing\nthat you may have to do.\n\nRationale:\nProject Ownership Having highest level of privileges on a project, to avoid misuse of project resources project\nownership assignment/change actions mentioned should be monitored and alerted to concerned recipients.\n- Sending project ownership Invites\n- Acceptance/Rejection of project ownership invite by user\n- Adding `role\\owner` to a user/service-account\n- Removing a user/Service account from `role\\owner`",
                    "impact": 1,
                    "title": "Ensure log metric filter and alerts exists for Project Ownership assignments/changes",
                    "source_location": {
                        "ref": "controls/logging_2_4.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_project_metrics with metric_filter == \"(protoPayload.serviceName=\\\"cloudresourcemanager.googleapis.com\\\") AND (ProjectOwnership OR projectOwnerInvitee) OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=\\\"REMOVE\\\" AND\\nprotoPayload.serviceData.policyDelta.bindingDeltas.role=\\\"roles/owner\\\") OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=\\\"ADD\\\" AND protoPayload.serviceData.policyDelta.bindingDeltas.role=\\\"roles/owner\\\")\\n\" ",
                            "run_time": 0.000038999,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "false Alert policy for filter \"(protoPayload.serviceName=\"cloudresourcemanager.googleapis.com\") AND (ProjectOwnership OR projectOwnerInvitee) OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=\"REMOVE\" AND\nprotoPayload.serviceData.policyDelta.bindingDeltas.role=\"roles/owner\") OR (protoPayload.serviceData.policyDelta.bindingDeltas.action=\"ADD\" AND protoPayload.serviceData.policyDelta.bindingDeltas.role=\"roles/owner\")\n\" does not exist",
                            "run_time": 0.000048728,
                            "start_time": "",
                            "message": "undefined method `expect' for #<RSpec::ExampleGroups::False_3:0x000000000be60fb8>",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.5",
                    "code": "control 'cis-gcp-benchmark-logging-2.5' do\n  impact 1.0\n  title 'Ensure log metric filter and alerts exists for Audit Configuration Changes'\n  desc '\n  Google Cloud Platform services write audit log entries to Admin Activity and Data Access logs to helps answer\n  the questions of \"who did what, where, and when?\" within Google Cloud Platform projects. Cloud Audit logging\n  records information includes the identity of the API caller, the time of the API call, the source IP address of\n  the API caller, the request parameters, and the response elements returned by the GCP services. Cloud Audit\n  logging provides a history of AWS API calls for an account, including API calls made via the Console, SDKs,\n  command line tools, and other GCP services.\n\n  Rationale:\n  Admin activity and Data access logs produced by Cloud audit logging enables security analysis, resource change\n  tracking, and compliance auditing. Configuring metric filter and alerts for Audit Configuration Changes ensures\n  recommended state of audit configuration and hence, all the activities in project are audit-able at any\n  point in time.\n  '\n  tag cis: 'gcp:2.5'\n  tag level: 1\n\n  filter = \"protoPayload.methodName=\\\"SetIamPolicy\\\" AND\\nprotoPayload.serviceData.policyDelta.auditConfigDeltas:*\\n\"\n  describe google_project_metrics(project: gcp_project_id).where(metric_filter: filter) do\n    it { should exist }\n  end\n\n  alert_policy_exists = false\n  google_project_metrics(project: gcp_project_id).where(metric_filter: filter).metric_types.each do |metric_type|\n    metric_filter = \"metric.type=\\\"#{metric_type}\\\" project=\\\"#{gcp_project_id}\\\"\"\n    google_project_alert_policies(project: gcp_project_id).where(policy_enabled_state: true).where { policy_filter_list.include?(metric_filter) }.policy_names.each do |policy_name|\n      describe google_project_alert_policy_condition(policy: policy_name, filter: metric_filter) do\n        alert_policy_exists=true\n        it { should exist }\n        its('condition_threshold_value') { should eq 0.001 }\n        its('aggregation_alignment_period') { should eq '60s' }\n        its('aggregation_cross_series_reducer') { should eq 'REDUCE_COUNT' }\n        its('aggregation_per_series_aligner') { should eq 'ALIGN_RATE' }\n      end\n    end\n  end\n\n  describe alert_policy_exists do\n    it \"Alert policy for filter \\\"#{filter}\\\" does not exist\" do\n      expect(alert_policy_exists).to(be true)\n    end\n  end\nend\n",
                    "desc": "Google Cloud Platform services write audit log entries to Admin Activity and Data Access logs to helps answer\nthe questions of \"who did what, where, and when?\" within Google Cloud Platform projects. Cloud Audit logging\nrecords information includes the identity of the API caller, the time of the API call, the source IP address of\nthe API caller, the request parameters, and the response elements returned by the GCP services. Cloud Audit\nlogging provides a history of AWS API calls for an account, including API calls made via the Console, SDKs,\ncommand line tools, and other GCP services.\n\nRationale:\nAdmin activity and Data access logs produced by Cloud audit logging enables security analysis, resource change\ntracking, and compliance auditing. Configuring metric filter and alerts for Audit Configuration Changes ensures\nrecommended state of audit configuration and hence, all the activities in project are audit-able at any\npoint in time.",
                    "impact": 1,
                    "title": "Ensure log metric filter and alerts exists for Audit Configuration Changes",
                    "source_location": {
                        "ref": "controls/logging_2_5.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_project_metrics with metric_filter == \"protoPayload.methodName=\\\"SetIamPolicy\\\" AND\\nprotoPayload.serviceData.policyDelta.auditConfigDeltas:*\\n\" ",
                            "run_time": 0.000042829,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "false Alert policy for filter \"protoPayload.methodName=\"SetIamPolicy\" AND\nprotoPayload.serviceData.policyDelta.auditConfigDeltas:*\n\" does not exist",
                            "run_time": 0.000045528,
                            "start_time": "",
                            "message": "undefined method `expect' for #<RSpec::ExampleGroups::False_4:0x000000000be3a200>",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.6",
                    "code": "control 'cis-gcp-benchmark-logging-2.6' do\n  impact 1.0\n  title 'Ensure log metric filter and alerts exists for Custom Role changes'\n  desc '\n  It is recommended that a metric filter and alarm be established for changes IAM Role creation,\n  deletion and updating activities.\n\n  Rationale:\n  Google Cloud Identity and Access Management (Cloud IAM) provides predefined roles that give granular access\n  to specific Google Cloud Platform resources and prevent unwanted access to other resources. However to cater\n  organization specific needs, Cloud IAM also provides ability to create custom roles. Project Owner and\n  administrators with Organization Role Administrator role or the IAM Role Administrator role can create custom\n  roles. Monitoring role creation, deletion and updating activities will help in identifying over-privileged\n  role at early stages.\n  '\n  tag cis: 'gcp:2.6'\n  tag level: 1\n\n  filter = \"resource.type=\\\"iam_role\\\" AND protoPayload.methodName = \\\"google.iam.admin.v1.CreateRole\\\" OR protoPayload.methodName=\\\"google.iam.admin.v1.DeleteRole\\\" OR protoPayload.methodName=\\\"google.iam.admin.v1.UpdateRole\\\"\\n\"\n  describe google_project_metrics(project: gcp_project_id).where(metric_filter: filter) do\n    it { should exist }\n  end\n\n  alert_policy_exists = false\n  google_project_metrics(project: gcp_project_id).where(metric_filter: filter).metric_types.each do |metric_type|\n    metric_filter = \"metric.type=\\\"#{metric_type}\\\" project=\\\"#{gcp_project_id}\\\"\"\n    google_project_alert_policies(project: gcp_project_id).where(policy_enabled_state: true).where { policy_filter_list.include?(metric_filter) }.policy_names.each do |policy_name|\n      describe google_project_alert_policy_condition(policy: policy_name, filter: metric_filter) do\n        alert_policy_exists=true\n        it { should exist }\n        its('condition_threshold_value') { should eq 0.001 }\n        its('aggregation_alignment_period') { should eq '60s' }\n        its('aggregation_cross_series_reducer') { should eq 'REDUCE_COUNT' }\n        its('aggregation_per_series_aligner') { should eq 'ALIGN_RATE' }\n      end\n    end\n  end\n\n  describe alert_policy_exists do\n    it \"Alert policy for filter \\\"#{filter}\\\" does not exist\" do\n      expect(alert_policy_exists).to(be true)\n    end\n  end\nend\n",
                    "desc": "It is recommended that a metric filter and alarm be established for changes IAM Role creation,\ndeletion and updating activities.\n\nRationale:\nGoogle Cloud Identity and Access Management (Cloud IAM) provides predefined roles that give granular access\nto specific Google Cloud Platform resources and prevent unwanted access to other resources. However to cater\norganization specific needs, Cloud IAM also provides ability to create custom roles. Project Owner and\nadministrators with Organization Role Administrator role or the IAM Role Administrator role can create custom\nroles. Monitoring role creation, deletion and updating activities will help in identifying over-privileged\nrole at early stages.",
                    "impact": 1,
                    "title": "Ensure log metric filter and alerts exists for Custom Role changes",
                    "source_location": {
                        "ref": "controls/logging_2_6.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_project_metrics with metric_filter == \"resource.type=\\\"iam_role\\\" AND protoPayload.methodName = \\\"google.iam.admin.v1.CreateRole\\\" OR protoPayload.methodName=\\\"google.iam.admin.v1.DeleteRole\\\" OR protoPayload.methodName=\\\"google.iam.admin.v1.UpdateRole\\\"\\n\" ",
                            "run_time": 0.000114169,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "false Alert policy for filter \"resource.type=\"iam_role\" AND protoPayload.methodName = \"google.iam.admin.v1.CreateRole\" OR protoPayload.methodName=\"google.iam.admin.v1.DeleteRole\" OR protoPayload.methodName=\"google.iam.admin.v1.UpdateRole\"\n\" does not exist",
                            "run_time": 0.000048206,
                            "start_time": "",
                            "message": "undefined method `expect' for #<RSpec::ExampleGroups::False_5:0x000000000bd07248>",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.7",
                    "code": "control 'cis-gcp-benchmark-logging-2.7' do\n  impact 1.0\n  title 'Ensure log metric filter and alerts exists for VPC Network Firewall rule changes'\n  desc '\n  It is recommended that a metric filter and alarm be established for VPC Network Firewall rule changes.\n\n  Rationale:\n  Monitoring for Create or Update firewall rule events gives insight network access changes and may reduce\n  the time it takes to detect suspicious activity.\n  '\n  tag cis: 'gcp:2.7'\n  tag level: 1\n\n  filter = 'resource.type=\"gce_firewall_rule\" AND jsonPayload.event_subtype=\"compute.firewalls.patch\" OR jsonPayload.event_subtype=\"compute.firewalls.insert\"'\n  describe google_project_metrics(project: gcp_project_id).where(metric_filter: filter) do\n    it { should exist }\n  end\n\n  alert_policy_exists = false\n  google_project_metrics(project: gcp_project_id).where(metric_filter: filter).metric_types.each do |metric_type|\n    metric_filter = \"metric.type=\\\"#{metric_type}\\\" project=\\\"#{gcp_project_id}\\\"\"\n    google_project_alert_policies(project: gcp_project_id).where(policy_enabled_state: true).where { policy_filter_list.include?(metric_filter) }.policy_names.each do |policy_name|\n      describe google_project_alert_policy_condition(policy: policy_name, filter: metric_filter) do\n        alert_policy_exists=true\n        it { should exist }\n        its('condition_threshold_value') { should eq 0.001 }\n        its('aggregation_alignment_period') { should eq '60s' }\n        its('aggregation_cross_series_reducer') { should eq 'REDUCE_COUNT' }\n        its('aggregation_per_series_aligner') { should eq 'ALIGN_RATE' }\n      end\n    end\n  end\n\n  describe alert_policy_exists do\n    it \"Alert policy for filter \\\"#{filter}\\\" does not exist\" do\n      expect(alert_policy_exists).to(be true)\n    end\n  end\nend\n",
                    "desc": "It is recommended that a metric filter and alarm be established for VPC Network Firewall rule changes.\n\nRationale:\nMonitoring for Create or Update firewall rule events gives insight network access changes and may reduce\nthe time it takes to detect suspicious activity.",
                    "impact": 1,
                    "title": "Ensure log metric filter and alerts exists for VPC Network Firewall rule changes",
                    "source_location": {
                        "ref": "controls/logging_2_7.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_project_metrics with metric_filter == \"resource.type=\\\"gce_firewall_rule\\\" AND jsonPayload.event_subtype=\\\"compute.firewalls.patch\\\" OR jsonPayload.event_subtype=\\\"compute.firewalls.insert\\\"\" ",
                            "run_time": 0.000041774,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "false Alert policy for filter \"resource.type=\"gce_firewall_rule\" AND jsonPayload.event_subtype=\"compute.firewalls.patch\" OR jsonPayload.event_subtype=\"compute.firewalls.insert\"\" does not exist",
                            "run_time": 0.000124959,
                            "start_time": "",
                            "message": "undefined method `expect' for #<RSpec::ExampleGroups::False_6:0x000000000bd042f0>",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.8",
                    "code": "control 'cis-gcp-benchmark-logging-2.8' do\n  impact 1.0\n  title 'Ensure log metric filter and alerts exists for VPC network route changes'\n  desc '\n  It is recommended that a metric filter and alarm be established for VPC network route changes.\n\n  Rationale:\n  Google Cloud Platform (GCP) routes define the paths network traffic takes from a VM instance to another\n  destinations. The other destination can be inside your VPC network (such as another VM) or outside of\n  it. Every route consists of a destination and a next hop. Traffic whose destination IP is within the destination\n  range is sent to the next hop for delivery.\n  Monitoring changes to route tables will help ensure that all VPC traffic flows through an expected path.\n  '\n  tag cis: 'gcp:2.8'\n  tag level: 1\n\n  filter = \"resource.type=\\\"gce_route\\\" AND jsonPayload.event_subtype=\\\"compute.routes.delete\\\" OR jsonPayload.event_subtype=\\\"compute.routes.insert\\\"\\n\"\n  describe google_project_metrics(project: gcp_project_id).where(metric_filter: filter) do\n    it { should exist }\n  end\n\n  alert_policy_exists = false\n  google_project_metrics(project: gcp_project_id).where(metric_filter: filter).metric_types.each do |metric_type|\n    metric_filter = \"metric.type=\\\"#{metric_type}\\\" project=\\\"#{gcp_project_id}\\\"\"\n    google_project_alert_policies(project: gcp_project_id).where(policy_enabled_state: true).where { policy_filter_list.include?(metric_filter) }.policy_names.each do |policy_name|\n      describe google_project_alert_policy_condition(policy: policy_name, filter: metric_filter) do\n        alert_policy_exists=true\n        it { should exist }\n        its('condition_threshold_value') { should eq 0.001 }\n        its('aggregation_alignment_period') { should eq '60s' }\n        its('aggregation_cross_series_reducer') { should eq 'REDUCE_COUNT' }\n        its('aggregation_per_series_aligner') { should eq 'ALIGN_RATE' }\n      end\n    end\n  end\n\n  describe alert_policy_exists do\n    it \"Alert policy for filter \\\"#{filter}\\\" does not exist\" do\n      expect(alert_policy_exists).to(be true)\n    end\n  end\nend\n",
                    "desc": "It is recommended that a metric filter and alarm be established for VPC network route changes.\n\nRationale:\nGoogle Cloud Platform (GCP) routes define the paths network traffic takes from a VM instance to another\ndestinations. The other destination can be inside your VPC network (such as another VM) or outside of\nit. Every route consists of a destination and a next hop. Traffic whose destination IP is within the destination\nrange is sent to the next hop for delivery.\nMonitoring changes to route tables will help ensure that all VPC traffic flows through an expected path.",
                    "impact": 1,
                    "title": "Ensure log metric filter and alerts exists for VPC network route changes",
                    "source_location": {
                        "ref": "controls/logging_2_8.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_project_metrics with metric_filter == \"resource.type=\\\"gce_route\\\" AND jsonPayload.event_subtype=\\\"compute.routes.delete\\\" OR jsonPayload.event_subtype=\\\"compute.routes.insert\\\"\\n\" ",
                            "run_time": 0.000040249,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "false Alert policy for filter \"resource.type=\"gce_route\" AND jsonPayload.event_subtype=\"compute.routes.delete\" OR jsonPayload.event_subtype=\"compute.routes.insert\"\n\" does not exist",
                            "run_time": 0.000046506,
                            "start_time": "",
                            "message": "undefined method `expect' for #<RSpec::ExampleGroups::False_7:0x000000000bd01348>",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-logging-2.9",
                    "code": "control 'cis-gcp-benchmark-logging-2.9' do\n  impact 1.0\n  title 'Ensure log metric filter and alerts exists for VPC network changes'\n  desc '\n  It is recommended that a metric filter and alarm be established for VPC network changes.\n\n  Rationale:\n  It is possible to have more than 1 VPC within an project, in addition it is also possible to create a\n  peer connection between 2 VPCs enabling network traffic to route between VPCs.\n  Monitoring changes to VPC will help ensure VPC traffic flow is not getting impacted.\n  '\n  tag cis: 'gcp:2.9'\n  tag level: 1\n\n  filter =\"resource.type=gce_network AND jsonPayload.event_subtype=\\\"compute.networks.insert\\\" OR jsonPayload.event_subtype=\\\"compute.networks.patch\\\" OR jsonPayload.event_subtype=\\\"compute.networks.delete\\\" OR jsonPayload.event_subtype=\\\"compute.networks.removePeering\\\" OR jsonPayload.event_subtype=\\\"compute.networks.addPeering\\\"\\n\"\n  describe google_project_metrics(project: gcp_project_id).where(metric_filter: filter) do\n    it { should exist }\n  end\n\n  alert_policy_exists = false\n  google_project_metrics(project: gcp_project_id).where(metric_filter: filter).metric_types.each do |metric_type|\n    metric_filter = \"metric.type=\\\"#{metric_type}\\\" project=\\\"#{gcp_project_id}\\\"\"\n    google_project_alert_policies(project: gcp_project_id).where(policy_enabled_state: true).where { policy_filter_list.include?(metric_filter) }.policy_names.each do |policy_name|\n      describe google_project_alert_policy_condition(policy: policy_name, filter: metric_filter) do\n        alert_policy_exists=true\n        it { should exist }\n        its('condition_threshold_value') { should eq 0.001 }\n        its('aggregation_alignment_period') { should eq '60s' }\n        its('aggregation_cross_series_reducer') { should eq 'REDUCE_COUNT' }\n        its('aggregation_per_series_aligner') { should eq 'ALIGN_RATE' }\n      end\n    end\n  end\n\n  describe alert_policy_exists do\n    it \"Alert policy for filter \\\"#{filter}\\\" does not exist\" do\n      expect(alert_policy_exists).to(be true)\n    end\n  end\nend\n",
                    "desc": "It is recommended that a metric filter and alarm be established for VPC network changes.\n\nRationale:\nIt is possible to have more than 1 VPC within an project, in addition it is also possible to create a\npeer connection between 2 VPCs enabling network traffic to route between VPCs.\nMonitoring changes to VPC will help ensure VPC traffic flow is not getting impacted.",
                    "impact": 1,
                    "title": "Ensure log metric filter and alerts exists for VPC network changes",
                    "source_location": {
                        "ref": "controls/logging_2_9.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_project_metrics with metric_filter == \"resource.type=gce_network AND jsonPayload.event_subtype=\\\"compute.networks.insert\\\" OR jsonPayload.event_subtype=\\\"compute.networks.patch\\\" OR jsonPayload.event_subtype=\\\"compute.networks.delete\\\" OR jsonPayload.event_subtype=\\\"compute.networks.removePeering\\\" OR jsonPayload.event_subtype=\\\"compute.networks.addPeering\\\"\\n\" ",
                            "run_time": 0.000039621,
                            "start_time": "",
                            "message": "The caller does not have permission",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "false Alert policy for filter \"resource.type=gce_network AND jsonPayload.event_subtype=\"compute.networks.insert\" OR jsonPayload.event_subtype=\"compute.networks.patch\" OR jsonPayload.event_subtype=\"compute.networks.delete\" OR jsonPayload.event_subtype=\"compute.networks.removePeering\" OR jsonPayload.event_subtype=\"compute.networks.addPeering\"\n\" does not exist",
                            "run_time": 0.000043615,
                            "start_time": "",
                            "message": "undefined method `expect' for #<RSpec::ExampleGroups::False_8:0x000000000bcd2368>",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-networking-3.1",
                    "code": "control 'cis-gcp-benchmark-networking-3.1' do\n  impact 1.0\n  title 'Ensure the default network does not exist in a project'\n  desc '\n  The default network has automatically created firewall rules and has\n  pre-fabricated network configuration. Based on your security and networking\n  requirements, you should create your network and delete the default network.\n  '\n  tag cis: 'gcp:3.1'\n  tag level: 1\n\n  describe google_compute_networks(project: gcp_project_id) do\n    its('network_names') { should_not include 'default' }\n  end\nend\n",
                    "desc": "The default network has automatically created firewall rules and has\npre-fabricated network configuration. Based on your security and networking\nrequirements, you should create your network and delete the default network.",
                    "impact": 1,
                    "title": "Ensure the default network does not exist in a project",
                    "source_location": {
                        "ref": "controls/networking_3_1.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_compute_networks network_names should not include \"default\"",
                            "run_time": 0.22451639,
                            "start_time": "",
                            "message": "expected [\"default\"] not to include \"default\"",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-networking-3.2",
                    "code": "control 'cis-gcp-benchmark-networking-3.2' do\n  impact 1.0\n  title 'Ensure legacy networks does not exists for a project'\n  desc '\n  In order to prevent use of legacy networks, a project should not have a legacy network configured.\n\n  Rationale:\n  Legacy networks have a single network IPv4 prefix range and a single gateway IP address for the whole\n  network. The network is global in scope and spans all cloud regions. You cannot create subnetworks in a\n  legacy network or switch from legacy to auto or custom subnet networks. Legacy networks can thus have an\n  impact for high network traffic projects and subject to the single point of contention or failure.\n  '\n  tag cis: 'gcp:3.2'\n  tag level: 1\n\n  google_compute_networks(project: gcp_project_id).network_names.each do |network_name|\n    describe google_compute_network(project: gcp_project_id, name: network_name) do\n      it { should_not be_legacy }\n    end\n  end\nend\n",
                    "desc": "In order to prevent use of legacy networks, a project should not have a legacy network configured.\n\nRationale:\nLegacy networks have a single network IPv4 prefix range and a single gateway IP address for the whole\nnetwork. The network is global in scope and spans all cloud regions. You cannot create subnetworks in a\nlegacy network or switch from legacy to auto or custom subnet networks. Legacy networks can thus have an\nimpact for high network traffic projects and subject to the single point of contention or failure.",
                    "impact": 1,
                    "title": "Ensure legacy networks does not exists for a project",
                    "source_location": {
                        "ref": "controls/networking_3_2.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "passed",
                            "code_desc": "Network default should not be legacy",
                            "run_time": 0.00193501,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-networking-3.3",
                    "code": "control 'cis-gcp-benchmark-networking-3.3' do\n  impact 1.0\n  title 'Ensure that DNSSEC is enabled for Cloud DNS'\n  desc '\n  Cloud DNS is a fast, reliable and cost-effective Domain Name System that powers millions of domains on the\n  internet. DNSSEC in Cloud DNS enables domain owners to take easy steps to protect their domains against DNS\n  hijacking and man-in-the-middle and other attacks.\n\n  Rationale:\n  Domain Name System Security Extensions (DNSSEC) adds security to the Domain Name System (DNS) protocol by\n  enabling DNS responses to be validated. Having a trustworthy Domain Name System (DNS) that translates a\n  domain name like www.example.com into its associated IP address is an increasingly important building block of\n  today\\'s web-based applications. Attackers can hijack this process of domain/IP lookup and redirect users to a\n  malicious site through DNS hijacking and man-in-the-middle attacks. DNSSEC helps mitigate the risk of such\n  attacks by cryptographically signing DNS records. As a result, it prevents attackers from issuing fake DNS\n  responses that may misdirect browsers to nefarious websites.\n  '\n  tag cis: 'gcp:3.3'\n  tag level: 1\n\n  describe google_dns_managed_zones(project: gcp_project_id).where(dnssec_config_state: false) do\n    it { should_not exist }\n  end\nend\n",
                    "desc": "Cloud DNS is a fast, reliable and cost-effective Domain Name System that powers millions of domains on the\ninternet. DNSSEC in Cloud DNS enables domain owners to take easy steps to protect their domains against DNS\nhijacking and man-in-the-middle and other attacks.\n\nRationale:\nDomain Name System Security Extensions (DNSSEC) adds security to the Domain Name System (DNS) protocol by\nenabling DNS responses to be validated. Having a trustworthy Domain Name System (DNS) that translates a\ndomain name like www.example.com into its associated IP address is an increasingly important building block of\ntoday's web-based applications. Attackers can hijack this process of domain/IP lookup and redirect users to a\nmalicious site through DNS hijacking and man-in-the-middle attacks. DNSSEC helps mitigate the risk of such\nattacks by cryptographically signing DNS records. As a result, it prevents attackers from issuing fake DNS\nresponses that may misdirect browsers to nefarious websites.",
                    "impact": 1,
                    "title": "Ensure that DNSSEC is enabled for Cloud DNS",
                    "source_location": {
                        "ref": "controls/networking_3_3.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_dns_managed_zones with dnssec_config_state == false ",
                            "run_time": 0.000095492,
                            "start_time": "",
                            "message": "Google Cloud DNS API has not been used in project 532037547248 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dns.googleapis.com/overview?project=532037547248 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-networking-3.4",
                    "code": "control 'cis-gcp-benchmark-networking-3.4' do\n  impact 1.0\n  title 'Ensure that RSASHA1 is not used for key-signing key in Cloud DNS DNSSEC'\n  desc '\n  DNSSEC algorithm numbers in this registry may be used in CERT RRs. Zone signing (DNSSEC) and transaction security\n  mechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms. The algorithm used for key signing\n  should be recommended one and it should not be weak.\n\n  Rationale:\n  DNSSEC algorithm numbers in this registry may be used in CERT RRs. Zonesigning (DNSSEC) and transaction security\n  mechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms.\n  The algorithm used for key signing should be recommended one and it should not be weak. When enabling DNSSEC for a\n  managed zone, or creating a managed zone with DNSSEC, you can select the DNSSEC signing algorithms and the\n  denial-of-existence type. Changing the DNSSEC settings is only effective for a managed zone if DNSSEC is not\n  already enabled. If you need to change the settings for a managed zone where it has been enabled, you can turn\n  DNSSEC off and then re-enable it with different settings.\n  '\n  tag cis: 'gcp:3.4'\n  tag level: 1\n\n  google_dns_managed_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    describe google_dns_managed_zone(project: gcp_project_id, zone: zone_name) do\n      its('key_signing_key_algorithm') { should_not eq 'rsasha1' }\n    end\n  end\nend\n",
                    "desc": "DNSSEC algorithm numbers in this registry may be used in CERT RRs. Zone signing (DNSSEC) and transaction security\nmechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms. The algorithm used for key signing\nshould be recommended one and it should not be weak.\n\nRationale:\nDNSSEC algorithm numbers in this registry may be used in CERT RRs. Zonesigning (DNSSEC) and transaction security\nmechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms.\nThe algorithm used for key signing should be recommended one and it should not be weak. When enabling DNSSEC for a\nmanaged zone, or creating a managed zone with DNSSEC, you can select the DNSSEC signing algorithms and the\ndenial-of-existence type. Changing the DNSSEC settings is only effective for a managed zone if DNSSEC is not\nalready enabled. If you need to change the settings for a managed zone where it has been enabled, you can turn\nDNSSEC off and then re-enable it with different settings.",
                    "impact": 1,
                    "title": "Ensure that RSASHA1 is not used for key-signing key in Cloud DNS DNSSEC",
                    "source_location": {
                        "ref": "controls/networking_3_4.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-networking-3.5",
                    "code": "control 'cis-gcp-benchmark-networking-3.5' do\n  impact 1.0\n  title 'Ensure that RSASHA1 is not used for zone-signing key in Cloud DNS DNSSEC'\n  desc '\n  DNSSEC algorithm numbers in this registry may be used in CERT RRs. Zone signing (DNSSEC) and transaction security\n  mechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms. The algorithm used for key signing\n  should be recommended one and it should not be weak.\n\n  Rationale:\n  DNSSEC algorithm numbers in this registry may be used in CERT RRs. Zonesigning (DNSSEC) and transaction security\n  mechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms.\n  The algorithm used for key signing should be recommended one and it should not be weak. When enabling DNSSEC for a\n  managed zone, or creating a managed zone with DNSSEC, you can select the DNSSEC signing algorithms and the\n  denial-of-existence type. Changing the DNSSEC settings is only effective for a managed zone if DNSSEC is not\n  already enabled. If you need to change the settings for a managed zone where it has been enabled, you can turn\n  DNSSEC off and then re-enable it with different settings.\n  '\n  tag cis: 'gcp:3.5'\n  tag level: 1\n\n  google_dns_managed_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    describe google_dns_managed_zone(project: gcp_project_id, zone: zone_name) do\n      its('zone_signing_key_algorithm') { should_not eq 'rsasha1' }\n    end\n  end\nend\n",
                    "desc": "DNSSEC algorithm numbers in this registry may be used in CERT RRs. Zone signing (DNSSEC) and transaction security\nmechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms. The algorithm used for key signing\nshould be recommended one and it should not be weak.\n\nRationale:\nDNSSEC algorithm numbers in this registry may be used in CERT RRs. Zonesigning (DNSSEC) and transaction security\nmechanisms (SIG(0) and TSIG) make use of particular subsets of these algorithms.\nThe algorithm used for key signing should be recommended one and it should not be weak. When enabling DNSSEC for a\nmanaged zone, or creating a managed zone with DNSSEC, you can select the DNSSEC signing algorithms and the\ndenial-of-existence type. Changing the DNSSEC settings is only effective for a managed zone if DNSSEC is not\nalready enabled. If you need to change the settings for a managed zone where it has been enabled, you can turn\nDNSSEC off and then re-enable it with different settings.",
                    "impact": 1,
                    "title": "Ensure that RSASHA1 is not used for zone-signing key in Cloud DNS DNSSEC",
                    "source_location": {
                        "ref": "controls/networking_3_5.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-networking-3.6",
                    "code": "control 'cis-gcp-benchmark-networking-3.6' do\n  only_if { cis_level == 2 }\n  impact 1.0\n  title 'Ensure that SSH access is restricted from the internet'\n  desc '\n  GCP Firewall Rules are specific to a VPC Network. Each rule either allows or denies traffic when its conditions\n  are met. Its conditions allow you to specify the type of traffic, such as ports and protocols, and the source or\n  destination of the traffic, including IP addresses, subnets, and instances. Firewall rules are defined at the\n  VPC network level, and are specific to the network in which they are defined. The rules themselves cannot be\n  shared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule\n  or a destination for an egress rule by address, you can only use an IPv4 address or IPv4 block in CIDR\n  notation. Generic (0.0.0.0/0) incoming traffic from internet to VPC or VM instance using SSH on Port 22 can\n  be avoided.\n\n  Rationale:\n  GCP Firewall Rules within a VPC Network. These rules apply to outgoing (egress) traffic from instances and\n  incoming (ingress) traffic to instances in the network. Egress and ingress traffic are controlled even if the\n  traffic stays within the network (for example, instance-to-instance communication). For an instance to have outgoing\n  Internet access, the network must have a valid Internet gateway route or custom route whose destination IP is\n  specified. This route simply defines the path to the Internet, to avoid the most general (0.0.0.0/0) destination\n  IP Range specified from Internet through SSH with default Port 22. We need to restrict generic access from\n  Internet to specific IP Range.\n  '\n  tag cis: 'gcp:3.6'\n  tag level: 2\n\n  # Here the spec asks for source range not to include 0.0.0.0/0 across INGRESS and EGRESS\n  google_compute_firewalls(project: gcp_project_id).firewall_names.each do |firewall_name|\n    describe google_compute_firewall(project: gcp_project_id, name: firewall_name) do\n      it { should_not allow_ip_ranges ['0.0.0.0/0'] }\n    end\n  end\n\n  # Specifically for INGRESS, disallow SSH on port 22\n  google_compute_firewalls(project: gcp_project_id).where(firewall_direction: 'INGRESS').firewall_names.each do |firewall_name|\n    describe google_compute_firewall(project: gcp_project_id, name: firewall_name) do\n      it { should_not be_allowed_ssh }\n    end\n  end\nend\n",
                    "desc": "GCP Firewall Rules are specific to a VPC Network. Each rule either allows or denies traffic when its conditions\nare met. Its conditions allow you to specify the type of traffic, such as ports and protocols, and the source or\ndestination of the traffic, including IP addresses, subnets, and instances. Firewall rules are defined at the\nVPC network level, and are specific to the network in which they are defined. The rules themselves cannot be\nshared among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule\nor a destination for an egress rule by address, you can only use an IPv4 address or IPv4 block in CIDR\nnotation. Generic (0.0.0.0/0) incoming traffic from internet to VPC or VM instance using SSH on Port 22 can\nbe avoided.\n\nRationale:\nGCP Firewall Rules within a VPC Network. These rules apply to outgoing (egress) traffic from instances and\nincoming (ingress) traffic to instances in the network. Egress and ingress traffic are controlled even if the\ntraffic stays within the network (for example, instance-to-instance communication). For an instance to have outgoing\nInternet access, the network must have a valid Internet gateway route or custom route whose destination IP is\nspecified. This route simply defines the path to the Internet, to avoid the most general (0.0.0.0/0) destination\nIP Range specified from Internet through SSH with default Port 22. We need to restrict generic access from\nInternet to specific IP Range.",
                    "impact": 1,
                    "title": "Ensure that SSH access is restricted from the internet",
                    "source_location": {
                        "ref": "controls/networking_3_6.rb",
                        "line": 8
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-http should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000398714,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-http.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-https should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000169302,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-https.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-icmp should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000149954,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-icmp.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-internal should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000160678,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-rdp should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.00016619,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-rdp.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-ssh should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000243998,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-ssh.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-http should not be allowed ssh",
                            "run_time": 0.000100046,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-https should not be allowed ssh",
                            "run_time": 0.00012072,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-icmp should not be allowed ssh",
                            "run_time": 0.000072183,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-internal should not be allowed ssh",
                            "run_time": 0.000200822,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-internal.allowed_ssh?` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-rdp should not be allowed ssh",
                            "run_time": 0.000071097,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-ssh should not be allowed ssh",
                            "run_time": 0.000192158,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-ssh.allowed_ssh?` to return false, got true",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-networking-3.7",
                    "code": "control 'cis-gcp-benchmark-networking-3.7' do\n  only_if { cis_level == 2 }\n  impact 1.0\n  title 'Ensure that RDP access is restricted from the internet'\n  desc '\n  GCP Firewall Rules are specific to a VPC Network. Each rule either allows or denies traffic when its conditions\n  are met. Its conditions allow you to specify the type of traffic, such as ports and protocols, and the source or\n  destination of the traffic, including IP addresses, subnets, and instances. Firewall rules are defined at the VPC\n  network level, and are specific to the network in which they are defined. The rules themselves cannot be shared\n  among networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a\n  destination for an egress rule by address, you can only use an IPv4 address or IPv4 block in CIDR\n  notation. Generic (0.0.0.0/0) incoming traffic from internet to VPC or VM instance using RDP on Port 3389\n  can be avoided.\n\n  Rationale:\n  GCP Firewall Rules within a VPC Network. These rules apply to outgoing (egress) traffic from instances and\n  incoming (ingress) traffic to instances in the network. Egress and ingress traffic are controlled even if the\n  traffic stays within the network (for example, instance-to-instance communication). For an instance to have\n  outgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP\n  is specified. This route simply defines the path to the Internet, to avoid the most general (0.0.0.0/0)\n  destination IP Range specified from Internet through RDP with default Port 3389. We need to restrict generic\n  access from Internet to specific IP Range.\n  '\n  tag cis: 'gcp:3.7'\n  tag level: 2\n\n  # Here the spec asks for source range not to include 0.0.0.0/0 across INGRESS and EGRESS\n  google_compute_firewalls(project: gcp_project_id).firewall_names.each do |firewall_name|\n    describe google_compute_firewall(project: gcp_project_id, name: firewall_name) do\n      it { should_not allow_ip_ranges ['0.0.0.0/0'] }\n    end\n  end\n\n  # Specifically for INGRESS, disallow RDP on port 3389\n  google_compute_firewalls(project: gcp_project_id).where(firewall_direction: 'INGRESS').firewall_names.each do |firewall_name|\n    describe google_compute_firewall(project: gcp_project_id, name: firewall_name) do\n      it { should_not be_allowed_rdp }\n    end\n  end\nend\n",
                    "desc": "GCP Firewall Rules are specific to a VPC Network. Each rule either allows or denies traffic when its conditions\nare met. Its conditions allow you to specify the type of traffic, such as ports and protocols, and the source or\ndestination of the traffic, including IP addresses, subnets, and instances. Firewall rules are defined at the VPC\nnetwork level, and are specific to the network in which they are defined. The rules themselves cannot be shared\namong networks. Firewall rules only support IPv4 traffic. When specifying a source for an ingress rule or a\ndestination for an egress rule by address, you can only use an IPv4 address or IPv4 block in CIDR\nnotation. Generic (0.0.0.0/0) incoming traffic from internet to VPC or VM instance using RDP on Port 3389\ncan be avoided.\n\nRationale:\nGCP Firewall Rules within a VPC Network. These rules apply to outgoing (egress) traffic from instances and\nincoming (ingress) traffic to instances in the network. Egress and ingress traffic are controlled even if the\ntraffic stays within the network (for example, instance-to-instance communication). For an instance to have\noutgoing Internet access, the network must have a valid Internet gateway route or custom route whose destination IP\nis specified. This route simply defines the path to the Internet, to avoid the most general (0.0.0.0/0)\ndestination IP Range specified from Internet through RDP with default Port 3389. We need to restrict generic\naccess from Internet to specific IP Range.",
                    "impact": 1,
                    "title": "Ensure that RDP access is restricted from the internet",
                    "source_location": {
                        "ref": "controls/networking_3_7.rb",
                        "line": 8
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-http should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000178372,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-http.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-https should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000161924,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-https.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-icmp should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000185042,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-icmp.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-internal should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000093315,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-rdp should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000243587,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-rdp.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-ssh should not allow ip ranges [\"0.0.0.0/0\"]",
                            "run_time": 0.000241615,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-ssh.allow_ip_ranges?([\"0.0.0.0/0\"])` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-http should not be allowed rdp",
                            "run_time": 0.00008193,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-https should not be allowed rdp",
                            "run_time": 0.000068772,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-icmp should not be allowed rdp",
                            "run_time": 0.000081123,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-internal should not be allowed rdp",
                            "run_time": 0.000136072,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-internal.allowed_rdp?` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Firewall Rule default-allow-rdp should not be allowed rdp",
                            "run_time": 0.000108141,
                            "start_time": "",
                            "message": "expected `Firewall Rule default-allow-rdp.allowed_rdp?` to return false, got true",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Firewall Rule default-allow-ssh should not be allowed rdp",
                            "run_time": 0.000069395,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-networking-3.8",
                    "code": "control 'cis-gcp-benchmark-networking-3.8' do\n  only_if { cis_level == 2 }\n  impact 1.0\n  title 'Ensure Private Google Access is enabled for all subnetwork in VPC Network'\n  desc '\n  Private Google Access enables virtual machine instances on a subnet to reach Google APIs and services using an\n  internal IP address rather than an external IP address. External IP addresses are routable and reachable over\n  the Internet. Internal (private) IP addresses are internal to Google Cloud Platform and are not routable or\n  reachable over the Internet. You can use Private Google Access to allow VMs without Internet access to reach\n  Google APIs, services, and properties that are accessible over HTTP/HTTPS.\n\n  Rationale:\n  VPC networks and subnetworks provide logically isolated and secure network partitions where you can launch GCP\n  resources. When Private Google Access is enabled, VM instances in a subnet can reach the Google Cloud and Developer\n  APIs and services without needing an external IP address. Instead, VMs can use their internal IP addresses to access\n  Google managed services. Instances with external IP addresses are not affected when you enable the ability to\n  access Google services from internal IP addresses. These instances can still connect to Google APIs and\n  managed services.\n  '\n  tag cis: 'gcp:3.8'\n  tag level: 2\n\n  google_compute_regions(project: gcp_project_id).region_names.each do |region_name|\n    google_compute_subnetworks(project: gcp_project_id, region: region_name).subnetwork_names.each do |subnetwork_name|\n      describe google_compute_subnetwork(project: gcp_project_id, region: region_name, name: subnetwork_name) do\n        its('private_ip_google_access') { should be true }\n      end\n    end\n  end\nend\n",
                    "desc": "Private Google Access enables virtual machine instances on a subnet to reach Google APIs and services using an\ninternal IP address rather than an external IP address. External IP addresses are routable and reachable over\nthe Internet. Internal (private) IP addresses are internal to Google Cloud Platform and are not routable or\nreachable over the Internet. You can use Private Google Access to allow VMs without Internet access to reach\nGoogle APIs, services, and properties that are accessible over HTTP/HTTPS.\n\nRationale:\nVPC networks and subnetworks provide logically isolated and secure network partitions where you can launch GCP\nresources. When Private Google Access is enabled, VM instances in a subnet can reach the Google Cloud and Developer\nAPIs and services without needing an external IP address. Instead, VMs can use their internal IP addresses to access\nGoogle managed services. Instances with external IP addresses are not affected when you enable the ability to\naccess Google services from internal IP addresses. These instances can still connect to Google APIs and\nmanaged services.",
                    "impact": 1,
                    "title": "Ensure Private Google Access is enabled for all subnetwork in VPC Network",
                    "source_location": {
                        "ref": "controls/networking_3_8.rb",
                        "line": 8
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.001410569,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000124502,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000104491,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000214033,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000098691,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.00010843,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000143807,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000110801,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000107638,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000140431,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000158747,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000109472,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000097792,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000094106,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000143728,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000099147,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000093635,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "Subnetwork default private_ip_google_access should equal true",
                            "run_time": 0.000174741,
                            "start_time": "",
                            "message": "\nexpected true\n     got false\n",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-networking-3.9",
                    "code": "control 'cis-gcp-benchmark-networking-3.9' do\n  impact 1.0\n  title 'Ensure VPC Flow logs is enabled for every subnet in VPC Network'\n  desc '\n  Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network\n  interfaces in your VPC Subnets. After you\\'ve created a flow log, you can view and retrieve its data in\n  Stackdriver Logging. It is recommended that Flow Logs be enabled for every business critical VPC subnet.\n\n  Rationale:\n  VPC networks and subnetworks provide logically isolated and secure network partitions where you can launch\n  GCP resources. When Flow Logs is enabled for a subnet, VMs within subnet starts reporting on all TCP and UDP\n  flows. Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from\n  another VM, a host in your on-premises datacenter, a Google service, or a host on the Internet. If two GCP\n  VMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.\n\n  Flow Logs supports following use cases:\n  - Network monitoring\n  - Understanding network usage and optimizing network traffic expenses\n  - Network forensics\n  - Real-time security analysis\n  Flow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect\n  anomalous traffic or insight during security workflows.\n  '\n  tag cis: 'gcp:3.9'\n  tag level: 1\n\n  google_compute_regions(project: gcp_project_id).region_names.each do |region_name|\n    describe google_compute_subnetworks(project: gcp_project_id, region: region_name).where(enable_flow_log: false) do\n      it { should_not exist }\n    end\n  end\nend\n",
                    "desc": "Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network\ninterfaces in your VPC Subnets. After you've created a flow log, you can view and retrieve its data in\nStackdriver Logging. It is recommended that Flow Logs be enabled for every business critical VPC subnet.\n\nRationale:\nVPC networks and subnetworks provide logically isolated and secure network partitions where you can launch\nGCP resources. When Flow Logs is enabled for a subnet, VMs within subnet starts reporting on all TCP and UDP\nflows. Each VM samples the TCP and UDP flows it sees, inbound and outbound, whether the flow is to or from\nanother VM, a host in your on-premises datacenter, a Google service, or a host on the Internet. If two GCP\nVMs are communicating, and both are in subnets that have VPC Flow Logs enabled, both VMs report the flows.\n\nFlow Logs supports following use cases:\n- Network monitoring\n- Understanding network usage and optimizing network traffic expenses\n- Network forensics\n- Real-time security analysis\nFlow Logs provide visibility into network traffic for each VM inside the subnet and can be used to detect\nanomalous traffic or insight during security workflows.",
                    "impact": 1,
                    "title": "Ensure VPC Flow logs is enabled for every subnet in VPC Network",
                    "source_location": {
                        "ref": "controls/networking_3_9.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.004612846,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.003495718,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.003223861,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.00308523,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.003056015,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002701453,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002595048,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002409039,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002381124,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002326277,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002315923,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002172848,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.00229838,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002225584,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002303316,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002148578,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002206183,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        },
                        {
                            "status": "failed",
                            "code_desc": "google_compute_subnetworks with enable_flow_log == false should not exist",
                            "run_time": 0.002030087,
                            "start_time": "",
                            "message": "expected google_compute_subnetworks with enable_flow_log == false not to exist",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-project-0.0",
                    "code": "control 'cis-gcp-benchmark-project-0.0' do\n  impact 1.0\n  title 'Ensure that the project exists and is in lifecycle state \"ACTIVE\".'\n\n  describe google_project(project: gcp_project_id) do\n    it { should exist }\n    its('lifecycle_state') { should eq 'ACTIVE' }\n  end\nend\n",
                    "desc": "",
                    "impact": 1,
                    "title": "Ensure that the project exists and is in lifecycle state \"ACTIVE\".",
                    "source_location": {
                        "ref": "controls/project_0_0.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "passed",
                            "code_desc": "Project dmurray-project should exist",
                            "run_time": 0.000232324,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        },
                        {
                            "status": "passed",
                            "code_desc": "Project dmurray-project lifecycle_state should eq \"ACTIVE\"",
                            "run_time": 0.000988146,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-storage-5.1",
                    "code": "control 'cis-gcp-benchmark-storage-5.1' do\n  impact 1.0\n  title 'Ensure that Cloud Storage bucket is not anonymously or publicly accessible'\n  desc '\n  It is recommended that IAM policy on Cloud Storage bucket does not allows anonymous and/or public access.\n\n  Rationale:\n  Allowing anonymous and/or public access grants permissions to anyone to access bucket content. Such access\n  might not be desired if you are storing any sensitive data. Hence, ensure that anonymous and/or public access\n  to a bucket is not allowed.\n  '\n  tag cis: 'gcp:5.1'\n  tag level: 1\n\n  google_storage_buckets(project: gcp_project_id).bucket_names.each do |bucket_name|\n    google_storage_bucket_iam_bindings(bucket: bucket_name).iam_binding_roles.each do |iam_binding_role|\n      describe google_storage_bucket_iam_binding(bucket: bucket_name, role: iam_binding_role) do\n        its('members') { should_not include 'allUsers' }\n        its('members') { should_not include 'allAuthenticatedUsers' }\n      end\n    end\n  end\nend\n",
                    "desc": "It is recommended that IAM policy on Cloud Storage bucket does not allows anonymous and/or public access.\n\nRationale:\nAllowing anonymous and/or public access grants permissions to anyone to access bucket content. Such access\nmight not be desired if you are storing any sensitive data. Hence, ensure that anonymous and/or public access\nto a bucket is not allowed.",
                    "impact": 1,
                    "title": "Ensure that Cloud Storage bucket is not anonymously or publicly accessible",
                    "source_location": {
                        "ref": "controls/storage_5_1.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-storage-5.3",
                    "code": "control 'cis-gcp-benchmark-storage-5.3' do\n  impact 1.0\n  title 'Ensure that logging is enabled for Cloud storage buckets'\n  desc '\n  Storage Access Logging generates a log that contains access records for each request made to the Storage\n  bucket. An access log record contains details about the request, such as the request type, the resources\n  specified in the request worked, and the time and date the request was processed. Cloud Storage offers access\n  logs and storage logs in the form of CSV files that can be downloaded and used for analysis/incident response.\n  Access logs provide information for all of the requests made on a specified bucket and are created hourly, while\n  the daily storage logs provide information about the storage consumption of that bucket for the last day. The access\n  logs and storage logs are automatically created as new objects in a bucket that you specify. An access log record\n  contains details about the request, such as the request type, the resources specified in the request worked, and\n  the time and date the request was processed. While storage Logs helps to keep track the amount of data stored in the\n  bucket. It is recommended that storage Access Logs and Storage logs are enabled for every Storage Bucket.\n\n  Rationale:\n  By enabling access and storage logs on target Storage buckets, it is possible to capture all events which may\n  affect objects within target buckets. Configuring logs to be placed in a separate bucket allows access to log\n  information which can be useful in security and incident response workflows.\n\n  In most cases, Cloud Audit Logging is the recommended method for generating logs that track API operations\n  performed in Cloud Storage:\n  - Cloud Audit Logging tracks access on a continuous basis.\n  - Cloud Audit Logging produces logs that are easier to work with.\n  - Cloud Audit Logging can monitor many of your Google Cloud Platform services, not just Cloud Storage.\n\n  In some cases, you may want to use access & storage logs instead. You most likely want to use access logs if:\n  - You want to track access for public objects.\n  - You use Access Control Lists (ACLs) to control access to your objects.\n  - You want to track changes made by the Object Lifecycle Management feature.\n  - You want your logs to include latency information, or the request and response size of individual HTTP requests.\n\n  You most likely want to use storage logs if:\n  - You want to track the amount of data stored in your buckets.\n  '\n  tag cis: 'gcp:5.3'\n  tag level: 1\n\n  google_storage_buckets(project: gcp_project_id).bucket_names.each do |bucket_name|\n    describe google_storage_bucket(name: bucket_name) do\n      it { should have_logging_enabled }\n    end\n  end\nend\n",
                    "desc": "Storage Access Logging generates a log that contains access records for each request made to the Storage\nbucket. An access log record contains details about the request, such as the request type, the resources\nspecified in the request worked, and the time and date the request was processed. Cloud Storage offers access\nlogs and storage logs in the form of CSV files that can be downloaded and used for analysis/incident response.\nAccess logs provide information for all of the requests made on a specified bucket and are created hourly, while\nthe daily storage logs provide information about the storage consumption of that bucket for the last day. The access\nlogs and storage logs are automatically created as new objects in a bucket that you specify. An access log record\ncontains details about the request, such as the request type, the resources specified in the request worked, and\nthe time and date the request was processed. While storage Logs helps to keep track the amount of data stored in the\nbucket. It is recommended that storage Access Logs and Storage logs are enabled for every Storage Bucket.\n\nRationale:\nBy enabling access and storage logs on target Storage buckets, it is possible to capture all events which may\naffect objects within target buckets. Configuring logs to be placed in a separate bucket allows access to log\ninformation which can be useful in security and incident response workflows.\n\nIn most cases, Cloud Audit Logging is the recommended method for generating logs that track API operations\nperformed in Cloud Storage:\n- Cloud Audit Logging tracks access on a continuous basis.\n- Cloud Audit Logging produces logs that are easier to work with.\n- Cloud Audit Logging can monitor many of your Google Cloud Platform services, not just Cloud Storage.\n\nIn some cases, you may want to use access & storage logs instead. You most likely want to use access logs if:\n- You want to track access for public objects.\n- You use Access Control Lists (ACLs) to control access to your objects.\n- You want to track changes made by the Object Lifecycle Management feature.\n- You want your logs to include latency information, or the request and response size of individual HTTP requests.\n\nYou most likely want to use storage logs if:\n- You want to track the amount of data stored in your buckets.",
                    "impact": 1,
                    "title": "Ensure that logging is enabled for Cloud storage buckets",
                    "source_location": {
                        "ref": "controls/storage_5_3.rb",
                        "line": 7
                    },
                    "results": [],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-vms-4.1",
                    "code": "control 'cis-gcp-benchmark-vms-4.1' do\n  impact 1.0\n  title 'Ensure that instances are not configured to use the default service account with full access to all Cloud APIs'\n  desc '\n  To support principle of least privileges and prevent potential privilege escalation it is recommended that instances\n  are not assigned to default service account Compute Engine default service account with Scope Allow full access to\n  all Cloud APIs.\n\n  Rationale:\n  Along with ability to optionally create, manage and use user managed custom service accounts, Google Compute Engine\n  provides default service account Compute Engine default service account for an instances to access necessary cloud\n  services. Project Editor role is assigned to Compute Engine default service account hence, This service account has\n  almost all capabilities over all cloud services except billing. However, when Compute Engine default service account\n  assigned to an instance it can operate in 3 scopes.\n  1. Allow default access: Allows only minimum access required to run an Instance (Least Privileges)\n  2. Allow full access to all Cloud APIs: Allow full access to all the cloud APIs/Services (Too much access)\n  3. Set access for each API: Allows Instance administrator to choose only those APIs that are needed to perform\n     specific business functionality expected by instance\n\n  When an instance is configured with Compute Engine default service account with Scope Allow full access to all Cloud\n  APIs, based on IAM roles assigned to the user(s) accessing Instance, it may allow user to perform cloud\n  operations/API calls that user is not supposed to perform leading to successful privilege escalation.\n  '\n  tag cis: 'gcp:4.1'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_compute_instances(project: gcp_project_id, zone: zone_name).instance_names.each do |instance_name|\n      describe google_compute_instance(project: gcp_project_id, zone: zone_name, name: instance_name) do\n        its('service_account_scopes') { should_not include 'https://www.googleapis.com/auth/cloud-platform' }\n      end\n    end\n  end\nend\n",
                    "desc": "To support principle of least privileges and prevent potential privilege escalation it is recommended that instances\nare not assigned to default service account Compute Engine default service account with Scope Allow full access to\nall Cloud APIs.\n\nRationale:\nAlong with ability to optionally create, manage and use user managed custom service accounts, Google Compute Engine\nprovides default service account Compute Engine default service account for an instances to access necessary cloud\nservices. Project Editor role is assigned to Compute Engine default service account hence, This service account has\nalmost all capabilities over all cloud services except billing. However, when Compute Engine default service account\nassigned to an instance it can operate in 3 scopes.\n1. Allow default access: Allows only minimum access required to run an Instance (Least Privileges)\n2. Allow full access to all Cloud APIs: Allow full access to all the cloud APIs/Services (Too much access)\n3. Set access for each API: Allows Instance administrator to choose only those APIs that are needed to perform\n   specific business functionality expected by instance\n\nWhen an instance is configured with Compute Engine default service account with Scope Allow full access to all Cloud\nAPIs, based on IAM roles assigned to the user(s) accessing Instance, it may allow user to perform cloud\noperations/API calls that user is not supposed to perform leading to successful privilege escalation.",
                    "impact": 1,
                    "title": "Ensure that instances are not configured to use the default service account with full access to all Cloud APIs",
                    "source_location": {
                        "ref": "controls/vms_4_1.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "passed",
                            "code_desc": "Instance chef-automate service_account_scopes should not include \"https://www.googleapis.com/auth/cloud-platform\"",
                            "run_time": 0.000173405,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-vms-4.2",
                    "code": "control 'cis-gcp-benchmark-vms-4.2' do\n  impact 1.0\n  title 'Ensure \"Block Project-wide SSH keys\" enabled for VM instances'\n  desc '\n  It is recommended to user Instance specific SSH key(s) instead of using common/shared project-wide SSH key(s)\n  to access Instances.\n\n  Rationale:\n  Project-wide SSH keys are stored in Compute/Project-meta-data. Project wide SSH keys can be used to login\n  into all the instances within project. Using project-wide SSH keys eases the SSH key management but if compromised,\n  poses the security risk which can impact all the instances within project. It is recommended to use Instance\n  specific SSH keys which can limit the attack surface if the SSH keys are compromised.\n  '\n  tag cis: 'gcp:4.2'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_compute_instances(project: gcp_project_id, zone: zone_name).instance_names.each do |instance_name|\n      describe google_compute_instance(project: gcp_project_id, zone: zone_name, name: instance_name) do\n        its('block_project_ssh_keys') { should eq true }\n      end\n    end\n  end\nend\n",
                    "desc": "It is recommended to user Instance specific SSH key(s) instead of using common/shared project-wide SSH key(s)\nto access Instances.\n\nRationale:\nProject-wide SSH keys are stored in Compute/Project-meta-data. Project wide SSH keys can be used to login\ninto all the instances within project. Using project-wide SSH keys eases the SSH key management but if compromised,\nposes the security risk which can impact all the instances within project. It is recommended to use Instance\nspecific SSH keys which can limit the attack surface if the SSH keys are compromised.",
                    "impact": 1,
                    "title": "Ensure \"Block Project-wide SSH keys\" enabled for VM instances",
                    "source_location": {
                        "ref": "controls/vms_4_2.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Instance chef-automate block_project_ssh_keys should eq true",
                            "run_time": 0.000143647,
                            "start_time": "",
                            "message": "\nexpected: true\n     got: false\n\n(compared using ==)\n",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-vms-4.3",
                    "code": "control 'cis-gcp-benchmark-vms-4.3' do\n  impact 1.0\n  title 'Ensure oslogin is enabled for a Project'\n  desc '\n  Enabling OS login binds SSH certificates to IAM users and facilitates effective SSH certificate management.\n\n  Rationale:\n  Enabling osLogin ensures that SSH keys used to connect to instances are mapped with IAM users. Revoking access to\n  IAM user will revoke all the SSH keys associated with that particular user. It facilitates centralized and\n  automated SSH key pair management which is useful in handling cases like response to compromised SSH key\n  pairs and/or revocation of external/third-party/Vendor users.\n  '\n  tag cis: 'gcp:4.3'\n  tag level: 1\n\n  describe google_compute_project_info(project: gcp_project_id) do\n    it { should have_enabled_oslogin }\n  end\nend\n",
                    "desc": "Enabling OS login binds SSH certificates to IAM users and facilitates effective SSH certificate management.\n\nRationale:\nEnabling osLogin ensures that SSH keys used to connect to instances are mapped with IAM users. Revoking access to\nIAM user will revoke all the SSH keys associated with that particular user. It facilitates centralized and\nautomated SSH key pair management which is useful in handling cases like response to compromised SSH key\npairs and/or revocation of external/third-party/Vendor users.",
                    "impact": 1,
                    "title": "Ensure oslogin is enabled for a Project",
                    "source_location": {
                        "ref": "controls/vms_4_3.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Compute Project Info dmurray-project should have enabled oslogin",
                            "run_time": 0.00144511,
                            "start_time": "",
                            "message": "expected #has_enabled_oslogin? to return true, got false",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-vms-4.4",
                    "code": "control 'cis-gcp-benchmark-vms-4.4' do\n  impact 1.0\n  title 'Ensure \"Enable connecting to serial ports\" is not enabled for VM Instance'\n  desc '\n  Interacting with a serial port is often referred to as the serial console, which is similar to using a\n  terminal window, in that input and output is entirely in text mode and there is no graphical interface or\n  mouse support.\n\n  If you enable the interactive serial console on an instance, clients can attempt to connect to that instance from\n  any IP address. Therefore interactive serial console support should be disabled.\n\n  Rationale:\n  A virtual machine instance has four virtual serial ports. Interacting with a serial port is similar to using a\n  terminal window, in that input and output is entirely in text mode and there is no graphical interface or mouse\n  support. The instance\\'s operating system, BIOS, and other system-level entities often write output to the serial\n  ports, and can accept input such as commands or answers to prompts. Typically, these system-level entities use the\n  first serial port (port 1) and serial port 1 is often referred to as the serial console.\n\n  The interactive serial console does not support IP-based access restrictions such as IP whitelists. If you enable\n  the interactive serial console on an instance, clients can attempt to connect to that instance from any IP address.\n  This allows anybody to connect to that instance if they know the correct SSH key, username, project ID, zone, and\n  instance name.\n\n  Therefore interactive serial console support should be disabled.\n  '\n  tag cis: 'gcp:4.4'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_compute_instances(project: gcp_project_id, zone: zone_name).instance_names.each do |instance_name|\n      describe google_compute_instance(project: gcp_project_id, zone: zone_name, name: instance_name) do\n        it { should have_serial_port_disabled }\n      end\n    end\n  end\nend\n",
                    "desc": "Interacting with a serial port is often referred to as the serial console, which is similar to using a\nterminal window, in that input and output is entirely in text mode and there is no graphical interface or\nmouse support.\n\nIf you enable the interactive serial console on an instance, clients can attempt to connect to that instance from\nany IP address. Therefore interactive serial console support should be disabled.\n\nRationale:\nA virtual machine instance has four virtual serial ports. Interacting with a serial port is similar to using a\nterminal window, in that input and output is entirely in text mode and there is no graphical interface or mouse\nsupport. The instance's operating system, BIOS, and other system-level entities often write output to the serial\nports, and can accept input such as commands or answers to prompts. Typically, these system-level entities use the\nfirst serial port (port 1) and serial port 1 is often referred to as the serial console.\n\nThe interactive serial console does not support IP-based access restrictions such as IP whitelists. If you enable\nthe interactive serial console on an instance, clients can attempt to connect to that instance from any IP address.\nThis allows anybody to connect to that instance if they know the correct SSH key, username, project ID, zone, and\ninstance name.\n\nTherefore interactive serial console support should be disabled.",
                    "impact": 1,
                    "title": "Ensure \"Enable connecting to serial ports\" is not enabled for VM Instance",
                    "source_location": {
                        "ref": "controls/vms_4_4.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Instance chef-automate should have serial port disabled",
                            "run_time": 0.000180678,
                            "start_time": "",
                            "message": "expected #has_serial_port_disabled? to return true, got false",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-vms-4.5",
                    "code": "control 'cis-gcp-benchmark-vms-4.5' do\n  impact 1.0\n  title 'Ensure that IP forwarding is not enabled on Instances'\n  desc '\n  Compute Engine instance cannot forward a packet unless the source IP address of the packet matches the IP address\n  of the instance. Similarly, GCP won\\'t deliver a packet whose destination IP address is different than the IP\n  address of the instance receiving the packet. However, both capabilities are required if you want to use\n  instances to help route packets.\n\n  Forwarding of data packets should be disabled to prevent data loss or information disclosure.\n\n  Rationale:\n  Compute Engine instance cannot forward a packet unless the source IP address of the packet matches the IP\n  address of the instance. Similarly, GCP won\\'t deliver a packet whose destination IP address is different\n  than the IP address of the instance receiving the packet. However, both capabilities are required if you want\n  to use instances to help route packets. To enable this source and destination IP check, disable the canIpForward\n  field, which allows an instance to send and receive packets with non-matching destination or source IPs.\n  '\n  tag cis: 'gcp:4.5'\n  tag level: 1\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_compute_instances(project: gcp_project_id, zone: zone_name).instance_names.each do |instance_name|\n      describe google_compute_instance(project: gcp_project_id, zone: zone_name, name: instance_name) do\n        its('can_ip_forward') { should be false }\n      end\n    end\n  end\nend\n",
                    "desc": "Compute Engine instance cannot forward a packet unless the source IP address of the packet matches the IP address\nof the instance. Similarly, GCP won't deliver a packet whose destination IP address is different than the IP\naddress of the instance receiving the packet. However, both capabilities are required if you want to use\ninstances to help route packets.\n\nForwarding of data packets should be disabled to prevent data loss or information disclosure.\n\nRationale:\nCompute Engine instance cannot forward a packet unless the source IP address of the packet matches the IP\naddress of the instance. Similarly, GCP won't deliver a packet whose destination IP address is different\nthan the IP address of the instance receiving the packet. However, both capabilities are required if you want\nto use instances to help route packets. To enable this source and destination IP check, disable the canIpForward\nfield, which allows an instance to send and receive packets with non-matching destination or source IPs.",
                    "impact": 1,
                    "title": "Ensure that IP forwarding is not enabled on Instances",
                    "source_location": {
                        "ref": "controls/vms_4_5.rb",
                        "line": 7
                    },
                    "results": [
                        {
                            "status": "passed",
                            "code_desc": "Instance chef-automate can_ip_forward should equal false",
                            "run_time": 0.000116595,
                            "start_time": "",
                            "message": "",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                },
                {
                    "id": "cis-gcp-benchmark-vms-4.6",
                    "code": "control 'cis-gcp-benchmark-vms-4.6' do\n  only_if { cis_level == 2 }\n  impact 1.0\n  title 'Ensure that IP forwarding is not enabled on Instances'\n  desc '\n  Customer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. If you\n  supply your own encryption keys, Google uses your key to protect the Google-generated keys used to encrypt and\n  decrypt your data. By default, Google Compute Engine encrypts all data at rest. Compute Engine handles and manages\n  this encryption for you without any additional actions on your part. However, if you wanted to control and\n  manage this encryption yourself, you can provide your own encryption keys.\n\n  Rationale:\n  By default, Google Compute Engine encrypts all data at rest. Compute Engine handles and manages this encryption\n  for you without any additional actions on your part. However, if you wanted to control and manage this\n  encryption yourself, you can provide your own encryption keys.\n\n  If you provide your own encryption keys, Compute Engine uses your key to protect the Google-generated keys used\n  to encrypt and decrypt your data. Only users who can provide the correct key can use resources protected by a\n  customer-supplied encryption key.\n\n  Google does not store your keys on its servers and cannot access your protected data unless you provide the\n  key. This also means that if you forget or lose your key, there is no way for Google to recover the key or to\n  recover any data encrypted with the lost key.\n\n  At least business critical VMs should have VM disks encrypted with CSEK.\n  '\n  tag cis: 'gcp:4.6'\n  tag level: 2\n\n  google_compute_zones(project: gcp_project_id).zone_names.each do |zone_name|\n    google_compute_instances(project: gcp_project_id, zone: zone_name).instance_names.each do |instance_name|\n      describe google_compute_instance(project: gcp_project_id, zone: zone_name, name: instance_name) do\n        it { should have_disks_encrypted_with_csek }\n      end\n    end\n  end\nend\n",
                    "desc": "Customer-Supplied Encryption Keys (CSEK) are a feature in Google Cloud Storage and Google Compute Engine. If you\nsupply your own encryption keys, Google uses your key to protect the Google-generated keys used to encrypt and\ndecrypt your data. By default, Google Compute Engine encrypts all data at rest. Compute Engine handles and manages\nthis encryption for you without any additional actions on your part. However, if you wanted to control and\nmanage this encryption yourself, you can provide your own encryption keys.\n\nRationale:\nBy default, Google Compute Engine encrypts all data at rest. Compute Engine handles and manages this encryption\nfor you without any additional actions on your part. However, if you wanted to control and manage this\nencryption yourself, you can provide your own encryption keys.\n\nIf you provide your own encryption keys, Compute Engine uses your key to protect the Google-generated keys used\nto encrypt and decrypt your data. Only users who can provide the correct key can use resources protected by a\ncustomer-supplied encryption key.\n\nGoogle does not store your keys on its servers and cannot access your protected data unless you provide the\nkey. This also means that if you forget or lose your key, there is no way for Google to recover the key or to\nrecover any data encrypted with the lost key.\n\nAt least business critical VMs should have VM disks encrypted with CSEK.",
                    "impact": 1,
                    "title": "Ensure that IP forwarding is not enabled on Instances",
                    "source_location": {
                        "ref": "controls/vms_4_6.rb",
                        "line": 8
                    },
                    "results": [
                        {
                            "status": "failed",
                            "code_desc": "Instance chef-automate should have disks encrypted with csek",
                            "run_time": 0.000157821,
                            "start_time": "",
                            "message": "expected #has_disks_encrypted_with_csek? to return true, got false",
                            "skip_message": ""
                        }
                    ],
                    "refs": [],
                    "tags": {}
                }
            ],
            "attributes": [],
            "latest_version": "",
            "status": "loaded",
            "skip_message": ""
        }
    ],
    "job_id": ""
}